{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "velvet-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from collections import defaultdict\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D, GRU, Bidirectional\n",
    "from keras.layers import GlobalMaxPooling1D, LSTM, Dropout, SimpleRNN, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import concatenate\n",
    "from keras import activations, initializers, constraints\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1,l2, l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "from layers import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latest-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The proposed model, HENIN\n",
    "def HENIN(GCNXss_shape, GCNXpp_shape, reg=l2(5e-4), n_layers=2, lr=0.01, n_head=8, size_per_head=8, MAX_REV_LEN=80, MAX_REV_WORD_LEN=6, support=3):\n",
    "    \n",
    "    '''\n",
    "    Comment Encoding\n",
    "    '''\n",
    "    \n",
    "    ''' Capture reviews context correlation'''\n",
    "    ## word-level encoding\n",
    "    word_input = Input(shape=(None, 300), dtype='float32')\n",
    "    word_sa = Self_Attention(n_head, size_per_head)(word_input)\n",
    "    word_avg = GlobalAveragePooling1D()(word_sa)\n",
    "    wordEncoder = Model(word_input, word_avg)\n",
    "    \n",
    "    ## review-level encoding\n",
    "    content_input = Input(shape=(MAX_REV_LEN, MAX_REV_WORD_LEN, 300), dtype='float32')\n",
    "    content_word_encode = TimeDistributed(wordEncoder, name='word_seq_encoder')(content_input)\n",
    "    content_sa = Self_Attention(n_head, size_per_head)(content_word_encode)\n",
    "    contentSA_avg_pool = GlobalAveragePooling1D()(content_sa) # session embedding\n",
    "    \n",
    "    ''' Capture Post-Comment co-attention'''\n",
    "    post_words_input = Input(shape=(None, 300), dtype='float32')\n",
    "    post_lstm = Bidirectional(GRU(32, return_sequences=True))(post_words_input)\n",
    "    coAtt_vec = CoAttLayer(MAX_REV_LEN)([content_word_encode, post_lstm])\n",
    "    \n",
    "    '''\n",
    "    GCN\n",
    "    Session-Session Interaction Extractor\n",
    "    Adjacency: session-session\n",
    "    '''\n",
    "    G_ss = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(3)]\n",
    "    \n",
    "    X_ss = Input(shape=(GCNXss_shape,))\n",
    "    X_ss_emb = Dense(16, activation='relu')(X_ss)\n",
    "    \n",
    "    # Define GCN model architecture\n",
    "    H_ss = Dropout(0.2)(X_ss_emb)\n",
    "    for i in range(n_layers-1):\n",
    "        H_ss = GraphConvolution(16, support, activation='relu', kernel_regularizer=reg)([H_ss]+G_ss)\n",
    "        \n",
    "    H_ss = GraphConvolution(8, support, activation='softmax', kernel_regularizer=reg)([H_ss]+G_ss)\n",
    "    \n",
    "    '''\n",
    "    GCN\n",
    "    Post-Post Interaction Extractor\n",
    "    Adjacency: post-post\n",
    "    '''\n",
    "    G_pp = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(3)]\n",
    "    \n",
    "    X_pp = Input(shape=(GCNXpp_shape,))\n",
    "    X_pp_emb = Dense(16, activation='relu')(X_pp)\n",
    "    \n",
    "    # Define GCN model architecture\n",
    "    H_pp = Dropout(0.2)(X_pp_emb)\n",
    "    for i in range(n_layers-1):\n",
    "        H_pp = GraphConvolution(16, support, activation='relu', kernel_regularizer=reg)([H_pp]+G_pp)\n",
    "    H_pp = GraphConvolution(8, support, activation='softmax', kernel_regularizer=reg)([H_pp]+G_pp)\n",
    "     \n",
    "    '''\n",
    "    Concatenate Comment Encoding & GCN Embedding\n",
    "    '''\n",
    "    H = concatenate([contentSA_avg_pool, coAtt_vec, H_ss, H_pp])\n",
    "    Y = Dense(1, activation='sigmoid')(H)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs=[content_input]+[post_words_input]+[X_ss]+G_ss+[X_pp]+G_pp, outputs=Y)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01))\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acceptable-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load data\n",
    "'''\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4ModelVINE.pickle', 'rb') as f:\n",
    "    Dat4Model = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_usersVINE.pickle', 'rb') as f:\n",
    "    multi_hot_users = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all = Dat4Model['w2v_vec_all'] # features for HENIN\n",
    "y_all = Dat4Model['y_all'] # target for HENIN\n",
    "textFeat_all = Dat4Model['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN = w2v_vec_all.shape[2]\n",
    "MAX_REV_LEN = w2v_vec_all.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb = pad_sequences(w2v_vec_all[:,0,:,:], maxlen=MAX_REV_LEN, dtype='float32', padding='post') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "governmental-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross validating for HENIN model\n",
    "def HENIN_cv(graph, y, A, model, epochs):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=9999, shuffle=True)\n",
    "    iters = 0\n",
    "    \n",
    "    for train_index, test_index in skf.split(range(len(y)), y):\n",
    "        y_train, y_test, train_mask = Mask_y(y=y, train_ix=train_index, test_ix=test_index)\n",
    "        #y_train, y_test = Mask_y(y=y, train_ix=train_index, test_ix=test_index)\n",
    "        clf = model\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            clf.fit(graph, y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=1)\n",
    "            #if epoch%5==0:\n",
    "                #print(metrics(y[test_index], (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)[test_index]))\n",
    "        preds = (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)\n",
    "        \n",
    "        completePerform = metrics(y, preds) # Complete set performance\n",
    "        generalPerform = metrics(y[test_index], preds[test_index]) # test set performance\n",
    "        \n",
    "          \n",
    "        try:\n",
    "            if iters == 1:\n",
    "                CP = {k: v + [completePerform[k]] for k, v in CP.items()}\n",
    "                GP = {k: v + [generalPerform[k]] for k, v in GP.items()}\n",
    "            else:  \n",
    "                CP = {k: [v] + [completePerform[k]] for k, v in CP.items()}\n",
    "                GP = {k: [v] + [generalPerform[k]] for k, v in GP.items()}\n",
    "                iters += 1\n",
    "        except:\n",
    "            CP = completePerform\n",
    "            GP = generalPerform\n",
    "    \n",
    "    AvgCP = {k: '{:.3f}'.format(np.mean(v)) for k, v in CP.items()}\n",
    "    AvgGP = {k: '{:.3f}'.format(np.mean(v)) for k, v in GP.items()}\n",
    "    \n",
    "    return AvgCP, AvgGP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "substantial-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "import time\n",
    "\n",
    "ppA = genAdjacencyMatrix(textFeat_all[:,0,:], 'cosine')\n",
    "ssA = genAdjacencyMatrix(multi_hot_users, 'cosine')\n",
    "\n",
    "graph_ss = genGCNgraph(ssA, multi_hot_users)\n",
    "graph_pp = genGCNgraph(ppA, textFeat_all[:,0,:])\n",
    "\n",
    "graph = [w2v_vec_all]+[postEmb]+graph_ss+graph_pp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-ticket",
   "metadata": {},
   "source": [
    "# Hyper-params tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caroline-return",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 29051)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           464832      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           4816        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 16)           784         dropout_1[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_5 (GraphConvo (None, 16)           784         dropout_2[0][0]                  \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 16)           784         graph_convolution_1[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_6 (GraphConvo (None, 16)           784         graph_convolution_5[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_2 (Self_Attenti (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 64)     63936       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_3 (GraphConvo (None, 16)           784         graph_convolution_2[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_7 (GraphConvo (None, 16)           784         graph_convolution_6[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           self__attention_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_1 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_4 (GraphConvo (None, 8)            392         graph_convolution_3[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_8 (GraphConvo (None, 8)            392         graph_convolution_7[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 208)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 co_att_layer_1[0][0]             \n",
      "                                                                 graph_convolution_4[0][0]        \n",
      "                                                                 graph_convolution_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            209         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 623,015\n",
      "Trainable params: 623,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.7155\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6862\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6637\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6391\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6177\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6112\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5953\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5955\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5824\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5893\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5805\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5869\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5741\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5756\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5658\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5582\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5540\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5426\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5392\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5315\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5282\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5243\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5219\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5181\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5156\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5116\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5102\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5070\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5064\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5040\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5020\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5004\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4984\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4965\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4939\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4918\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4893\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4868\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4850\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4847\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4898\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5090\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4734\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4853\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4758\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4694\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4871\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4634\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4720\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4652\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4707\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4634\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4682\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4620\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4647\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4603\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4623\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4580\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4586\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4535\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4528\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4540\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4497\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4477\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4485\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4462\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4426\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4424\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4427\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4397\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4377\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4354\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4363\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4370\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4356\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4366\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4381\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4372\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4304\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4236\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4367\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4287\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4279\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4292\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4280\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4277\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4281\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4357\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4560\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4317\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4285\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4367\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4224\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4315\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4252\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4222\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4265\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4215\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4207\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4240\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4190\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4207\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4191\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4160\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4138\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4147\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4126\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4115\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4108\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4076\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4069\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4076\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4049\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4037\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4024\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4007\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4007\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4008\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3990\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4257\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4186\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4198\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4189\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4180\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4164\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4150\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4174\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4213\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4118\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4070\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4107\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4148\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4066\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4044\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 4ms/step - loss: 0.4080\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4099\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4051\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3995\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3993\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4013\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4045\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3997\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3987\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3935\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3906\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3952\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4027\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3996\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3886\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3833\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3829\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3943\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4213\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3987\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3866\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3965\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3832\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3797\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3851\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3940\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3885\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 4ms/step - loss: 0.3918\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3886\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3807\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3792\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3782\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3792\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3723\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3700\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3705\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3708\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3748\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3671\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3588\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3573\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3599\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3753\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3860\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3858\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3541\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3963\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4107\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3595\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3816\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3743\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3591\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3791\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3526\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3524\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3598\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3440\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3600\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3383\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3559\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3558\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3402\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3523\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3329\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3422\n",
      "i=4,l={'l1': 0.0, 'l2': 9.999999747378752e-05}, {'acc': '0.794', 'prec': '0.754', 'rec': '0.523', 'f1': '0.613'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 29051)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           464832      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4816        input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_9 (GraphConvo (None, 16)           784         dropout_3[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_14 (GraphConv (None, 16)           784         dropout_4[0][0]                  \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_10 (GraphConv (None, 16)           784         graph_convolution_9[0][0]        \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_15 (GraphConv (None, 16)           784         graph_convolution_14[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_11 (GraphConv (None, 16)           784         graph_convolution_10[0][0]       \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_16 (GraphConv (None, 16)           784         graph_convolution_15[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_4 (Self_Attenti (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 64)     63936       input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_12 (GraphConv (None, 16)           784         graph_convolution_11[0][0]       \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_17 (GraphConv (None, 16)           784         graph_convolution_16[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 64)           0           self__attention_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_2 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_13 (GraphConv (None, 8)            392         graph_convolution_12[0][0]       \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_18 (GraphConv (None, 8)            392         graph_convolution_17[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 208)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 co_att_layer_2[0][0]             \n",
      "                                                                 graph_convolution_13[0][0]       \n",
      "                                                                 graph_convolution_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            209         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 624,583\n",
      "Trainable params: 624,583\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 8ms/step - loss: 1.0229\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.9781\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.9452\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.9227\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.8634\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.8323\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.8013\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.7748\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.7588\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.7547\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.7299\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.7218\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6972\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6841\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6619\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6481\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6296\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6168\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6025\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5933\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5786\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5725\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5569\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5524\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5430\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5363\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5324\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5250\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5235\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5178\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5173\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5135\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5108\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5089\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5035\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4995\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4963\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4923\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4894\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4875\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4763\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4730\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4624\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4564\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4656\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4731\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4599\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4524\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4617\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4516\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4472\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4488\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4429\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4419\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4423\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4364\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4373\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4341\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4309\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4297\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4274\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4263\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4244\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4237\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4277\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4349\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4232\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4218\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4343\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4179\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4244\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4247\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4167\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4197\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4156\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4153\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4179\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4144\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4116\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4123\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4261\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4229\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4236\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4209\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4211\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4212\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4165\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4144\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4154\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4136\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4146\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4092\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4118\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4139\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4092\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4102\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4094\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4059\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4071\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4034\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4022\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4028\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3973\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3964\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3934\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3928\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3918\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3913\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3930\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4018\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4075\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3876\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3955\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4008\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3883\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3919\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3897\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3848\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3822\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4253\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4147\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4026\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4067\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4072\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4029\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4025\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3954\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3914\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3934\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3897\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3861\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3830\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3825\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3808\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3763\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3745\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3719\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3705\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3709\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3802\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3655\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3665\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4031\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3738\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3607\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3610\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3611\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3709\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3646\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3496\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3480\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3412\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3507\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3521\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3624\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3478\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3301\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3413\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3323\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3784\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3958\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3627\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3917\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3797\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3972\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4078\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3571\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3877\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3632\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3783\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3418\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3520\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3451\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3482\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3411\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3461\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3488\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3355\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3270\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3317\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3241\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3205\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3237\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3140\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3116\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3163\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3070\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3110\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3642\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3357\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3462\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3562\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3438\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3423\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3395\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3357\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3294\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3274\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3204\n",
      "i=5,l={'l1': 0.0003000000142492354, 'l2': 0.0}, {'acc': '0.805', 'prec': '0.739', 'rec': '0.586', 'f1': '0.651'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 29051)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_33 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           464832      input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4816        input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_31 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_19 (GraphConv (None, 16)           784         dropout_5[0][0]                  \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_23 (GraphConv (None, 16)           784         dropout_6[0][0]                  \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_20 (GraphConv (None, 16)           784         graph_convolution_19[0][0]       \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_24 (GraphConv (None, 16)           784         graph_convolution_23[0][0]       \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_6 (Self_Attenti (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, None, 64)     63936       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_21 (GraphConv (None, 16)           784         graph_convolution_20[0][0]       \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_25 (GraphConv (None, 16)           784         graph_convolution_24[0][0]       \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           self__attention_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_3 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_22 (GraphConv (None, 8)            392         graph_convolution_21[0][0]       \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_26 (GraphConv (None, 8)            392         graph_convolution_25[0][0]       \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 208)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 co_att_layer_3[0][0]             \n",
      "                                                                 graph_convolution_22[0][0]       \n",
      "                                                                 graph_convolution_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            209         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 623,015\n",
      "Trainable params: 623,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.7331\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.7102\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6890\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6706\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6362\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.6166\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.6081\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.6037\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.6026\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5912\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5850\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5765\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5678\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5582\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5485\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5384\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.5294\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5228\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5178\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5127\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5124\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5428\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5203\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5135\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5102\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5128\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4996\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5112\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4969\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5030\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.5004\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4940\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4993\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4909\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4933\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4905\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4876\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4887\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4837\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4855\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4715\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4726\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4690\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4682\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4671\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4650\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4661\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4628\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4634\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4628\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4597\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4601\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4600\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4579\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4563\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4564\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4568\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4573\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4571\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 4ms/step - loss: 0.4565\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4556\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4550\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4531\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4534\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4533\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4534\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4525\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4523\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4501\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4482\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4456\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4444\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4437\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4450\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4501\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4554\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4573\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4464\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4393\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4448\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4445\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4372\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4369\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4387\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4351\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4331\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4343\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4329\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4301\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4304\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4293\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4279\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4274\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4263\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4244\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4233\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4232\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4221\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4212\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4210\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4201\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4196\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4169\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4163\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4156\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4150\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4149\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4158\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4151\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4148\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4165\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4162\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4179\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4161\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4099\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4062\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4082\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4088\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4064\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4301\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4274\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4253\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4243\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4214\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4204\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4216\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4224\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4265\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4314\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4226\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4126\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4105\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4116\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4145\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4150\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4104\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4030\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4038\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4019\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3998\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.4011\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3988\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3970\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3950\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3948\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3948\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3962\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3895\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3847\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3816\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3875\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3952\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3950\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3839\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3731\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3710\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3801\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3928\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3979\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.4067\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3936\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3980\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3962\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3856\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3848\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3852\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3781\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3760\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3776\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3703\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3653\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3666\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3629\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3598\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3578\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3542\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3514\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3541\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3519\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3514\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3618\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3552\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3437\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3342\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3316\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3336\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3396\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3579\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3570\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3248\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3256\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3420\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3240\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3127\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3150\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 4s 5ms/step - loss: 0.3136\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3030\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3006\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 5s 5ms/step - loss: 0.3001\n",
      "i=4,l={'l1': 0.0, 'l2': 0.0003000000142492354}, {'acc': '0.801', 'prec': '0.733', 'rec': '0.579', 'f1': '0.644'}\n"
     ]
    }
   ],
   "source": [
    "vine_best_results={}\n",
    "\n",
    "#start=time.time()\n",
    "for param in [(4, l2(1e-4),lr=0.01), (4, l1(3e-4), lr=0.01), (4, l2(3e-4)), lr=0.003]:\n",
    "    clf = HENIN(GCNXss_shape=multi_hot_users.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all[:,0,:].shape[1], \n",
    "            reg=param[1], n_layers=param[0],lr=param[2],\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n",
    "\n",
    "    AvgCP, AvgGP = HENIN_cv(graph=graph, y=y_all, A=ppA, model=clf, epochs=40)\n",
    "    vine_best_results[tuple(param)] = AvgGP\n",
    "    print(f\"i={param[0]},l={param[1].get_config()}, {AvgGP}\")\n",
    "#print(f\"Total run time={time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "little-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers=4, reg={'l1': 0.0, 'l2': 9.999999747378752e-05}, result={'acc': '0.794', 'prec': '0.754', 'rec': '0.523', 'f1': '0.613'}\n",
      "layers=5, reg={'l1': 0.0003000000142492354, 'l2': 0.0}, result={'acc': '0.805', 'prec': '0.739', 'rec': '0.586', 'f1': '0.651'}\n",
      "layers=4, reg={'l1': 0.0, 'l2': 0.0003000000142492354}, result={'acc': '0.801', 'prec': '0.733', 'rec': '0.579', 'f1': '0.644'}\n"
     ]
    }
   ],
   "source": [
    "for k,v in vine_best_results.items():\n",
    "    print(f\"layers={k[0]}, reg={k[1].get_config()}, result={v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-london",
   "metadata": {},
   "source": [
    "# Early Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sapphire-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1 3rd of comments\n",
    "'''\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4Model_vine25.pickle', 'rb') as f:\n",
    "    Dat4Model_3rd = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_users_vine25.pickle', 'rb') as f:\n",
    "    multi_hot_users_3rd = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all_3rd = Dat4Model_3rd['w2v_vec_all'] # features for HENIN\n",
    "y_all_3rd = Dat4Model_3rd['y_all'] # target for HENIN\n",
    "textFeat_all_3rd = Dat4Model_3rd['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN_3rd = w2v_vec_all_3rd.shape[2]\n",
    "MAX_REV_LEN_3rd = w2v_vec_all_3rd.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb_3rd = pad_sequences(w2v_vec_all_3rd[:,0,:,:], maxlen=MAX_REV_LEN_3rd, dtype='float32', padding='post') \n",
    "\n",
    "ppA_3rd = genAdjacencyMatrix(textFeat_all_3rd[:,0,:], 'cosine')\n",
    "ssA_3rd = genAdjacencyMatrix(multi_hot_users_3rd, 'cosine')\n",
    "\n",
    "graph_ss_3rd = genGCNgraph(ssA_3rd, multi_hot_users_3rd)\n",
    "graph_pp_3rd = genGCNgraph(ppA_3rd, textFeat_all_3rd[:,0,:])\n",
    "\n",
    "graph_3rd = [w2v_vec_all_3rd]+[postEmb_3rd]+graph_ss_3rd+graph_pp_3rd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "accredited-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 6th of comments\n",
    "\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4Model_vine12.pickle', 'rb') as f:\n",
    "    Dat4Model_6th = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_users_vine12.pickle', 'rb') as f:\n",
    "    multi_hot_users_6th = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all_6th = Dat4Model_6th['w2v_vec_all'] # features for HENIN\n",
    "y_all_6th = Dat4Model_6th['y_all'] # target for HENIN\n",
    "textFeat_all_6th = Dat4Model_6th['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN_6th = w2v_vec_all_6th.shape[2]\n",
    "MAX_REV_LEN_6th = w2v_vec_all_6th.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb_6th = pad_sequences(w2v_vec_all_6th[:,0,:,:], maxlen=MAX_REV_LEN_6th, dtype='float32', padding='post') \n",
    "\n",
    "ppA_6th = genAdjacencyMatrix(textFeat_all_6th[:,0,:], 'cosine')\n",
    "ssA_6th = genAdjacencyMatrix(multi_hot_users_6th, 'cosine')\n",
    "\n",
    "graph_ss_6th = genGCNgraph(ssA_6th, multi_hot_users_6th)\n",
    "graph_pp_6th = genGCNgraph(ppA_6th, textFeat_all_6th[:,0,:])\n",
    "\n",
    "graph_6th = [w2v_vec_all_6th]+[postEmb_6th]+graph_ss_6th+graph_pp_6th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "transparent-studio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           (None, 29051)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_33 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 16)           464832      input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 16)           4816        input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 16)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_28 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 16)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_31 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_32 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_24 (InputLayer)           (None, 25, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_17 (GraphConv (None, 16)           784         dropout_5[0][0]                  \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_21 (GraphConv (None, 16)           784         dropout_6[0][0]                  \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 25, 64)       57600       input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_25 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_18 (GraphConv (None, 16)           784         graph_convolution_17[0][0]       \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_22 (GraphConv (None, 16)           784         graph_convolution_21[0][0]       \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_6 (Self_Attenti (None, 25, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, None, 64)     63936       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_19 (GraphConv (None, 16)           784         graph_convolution_18[0][0]       \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_23 (GraphConv (None, 16)           784         graph_convolution_22[0][0]       \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           self__attention_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_3 (CoAttLayer)     (None, 128)          7346        word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_20 (GraphConv (None, 8)            392         graph_convolution_19[0][0]       \n",
      "                                                                 input_26[0][0]                   \n",
      "                                                                 input_27[0][0]                   \n",
      "                                                                 input_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_24 (GraphConv (None, 8)            392         graph_convolution_23[0][0]       \n",
      "                                                                 input_30[0][0]                   \n",
      "                                                                 input_31[0][0]                   \n",
      "                                                                 input_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 208)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 co_att_layer_3[0][0]             \n",
      "                                                                 graph_convolution_20[0][0]       \n",
      "                                                                 graph_convolution_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            209         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 616,515\n",
      "Trainable params: 616,515\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.7154\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.6776\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.6669\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.6412\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.6311\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.6213\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5983\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.6130\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5820\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5834\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5770\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5570\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5576\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5373\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5420\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5228\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5346\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5175\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5297\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5177\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5210\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5126\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5133\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5075\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5087\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5020\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5048\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4989\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5023\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4970\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.5001\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4960\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4981\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4954\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4962\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4947\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4947\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4940\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4935\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4931\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4797\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4785\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4782\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4767\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4766\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4747\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4748\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4728\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4722\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4714\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4696\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4689\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4686\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4676\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4660\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4649\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4643\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4641\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4653\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4684\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4730\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4679\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4605\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4614\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4653\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4622\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4574\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4602\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4614\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4563\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4560\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4588\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4561\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4529\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4539\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4550\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4530\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4504\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4506\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4519\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4525\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4512\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4479\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4466\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4478\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4479\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4461\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4437\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4428\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4430\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4432\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4426\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4409\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4388\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4379\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4373\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4375\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4382\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4401\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4442\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4498\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4459\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4310\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4352\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4377\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4324\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4273\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4285\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4340\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4349\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4292\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4231\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4236\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4282\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4292\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4266\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4194\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4186\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4234\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4495\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4435\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4377\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4382\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4428\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4403\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4388\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4391\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4400\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4377\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4348\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4346\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4323\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4300\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4305\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4304\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4288\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4272\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4268\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4256\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4247\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4235\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4223\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4217\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4234\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4275\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4423\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4539\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4311\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4185\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4259\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4293\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4195\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4186\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4203\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4184\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4140\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4135\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4158\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4137\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4201\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4251\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4228\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4191\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4164\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4192\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4236\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4271\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4182\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4155\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4159\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4193\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4254\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4200\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4110\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4072\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4100\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4105\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4086\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4039\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4028\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4027\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4006\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4016\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3978\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3969\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3978\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3972\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4054\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4033\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4179\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.4093\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3910\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3957\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3981\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3992\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3956\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3960\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3842\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 3s 3ms/step - loss: 0.3827\n",
      " result={'acc': '0.790', 'prec': '0.720', 'rec': '0.547', 'f1': '0.617'}\n"
     ]
    }
   ],
   "source": [
    "clf_3rd = HENIN(GCNXss_shape=multi_hot_users_3rd.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all_3rd[:,0,:].shape[1], \n",
    "            reg=l2(1e-4), n_layers=4, lr=0.01,\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN_3rd, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN_3rd, support=3)\n",
    "\n",
    "AvgCP_3rd, AvgGP_3rd = HENIN_cv(graph=graph_3rd, y=y_all_3rd, A=ppA_3rd, model=clf_3rd, epochs=40)\n",
    "#best_results[tuple(param)] = AvgGP\n",
    "print(f\" result={AvgGP_3rd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "domestic-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_40 (InputLayer)           (None, 29051)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16)           464832      input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16)           4816        input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 16)           0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_37 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_38 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_39 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 16)           0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_41 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_43 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_35 (InputLayer)           (None, 12, 6, 300)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_25 (GraphConv (None, 16)           784         dropout_7[0][0]                  \n",
      "                                                                 input_37[0][0]                   \n",
      "                                                                 input_38[0][0]                   \n",
      "                                                                 input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_29 (GraphConv (None, 16)           784         dropout_8[0][0]                  \n",
      "                                                                 input_41[0][0]                   \n",
      "                                                                 input_42[0][0]                   \n",
      "                                                                 input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 12, 64)       57600       input_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_36 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_26 (GraphConv (None, 16)           784         graph_convolution_25[0][0]       \n",
      "                                                                 input_37[0][0]                   \n",
      "                                                                 input_38[0][0]                   \n",
      "                                                                 input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_30 (GraphConv (None, 16)           784         graph_convolution_29[0][0]       \n",
      "                                                                 input_41[0][0]                   \n",
      "                                                                 input_42[0][0]                   \n",
      "                                                                 input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_8 (Self_Attenti (None, 12, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, None, 64)     63936       input_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_27 (GraphConv (None, 16)           784         graph_convolution_26[0][0]       \n",
      "                                                                 input_37[0][0]                   \n",
      "                                                                 input_38[0][0]                   \n",
      "                                                                 input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_31 (GraphConv (None, 16)           784         graph_convolution_30[0][0]       \n",
      "                                                                 input_41[0][0]                   \n",
      "                                                                 input_42[0][0]                   \n",
      "                                                                 input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 64)           0           self__attention_8[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_4 (CoAttLayer)     (None, 128)          5656        word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_28 (GraphConv (None, 8)            392         graph_convolution_27[0][0]       \n",
      "                                                                 input_37[0][0]                   \n",
      "                                                                 input_38[0][0]                   \n",
      "                                                                 input_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_32 (GraphConv (None, 8)            392         graph_convolution_31[0][0]       \n",
      "                                                                 input_41[0][0]                   \n",
      "                                                                 input_42[0][0]                   \n",
      "                                                                 input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 208)          0           global_average_pooling1d_8[0][0] \n",
      "                                                                 co_att_layer_4[0][0]             \n",
      "                                                                 graph_convolution_28[0][0]       \n",
      "                                                                 graph_convolution_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            209         concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 614,825\n",
      "Trainable params: 614,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.7120\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.6760\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.6658\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.6676\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.6491\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.6494\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.6359\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.6278\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.6264\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.6110\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.6056\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5968\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5792\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.5743\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5558\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5563\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5398\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5458\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5388\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.5273\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5371\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5285\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.5189\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5308\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5172\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5210\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5177\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5144\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5162\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.5112\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5134\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.5084\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5102\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5067\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5074\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5052\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.5050\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5043\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5030\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5034\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5019\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5019\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5005\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5008\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4993\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4994\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4979\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4979\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4966\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4963\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4954\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4945\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.4942\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4928\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4925\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4914\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4904\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4900\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4889\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4878\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.4873\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4864\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.4852\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4844\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4838\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 3ms/step - loss: 0.4834\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4826\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4818\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4809\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4799\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4790\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4782\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4773\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4769\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4775\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4806\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4882\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4860\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4757\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4739\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4897\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4850\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4791\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4855\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4799\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4776\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4813\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4750\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4762\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4759\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4714\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4731\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4718\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4683\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4690\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4699\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4680\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4653\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4647\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4655\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4656\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4645\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4602\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4591\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4587\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4590\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4594\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4610\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4627\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4658\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4629\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4569\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4499\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4489\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4523\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4535\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4515\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4454\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4408\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4497\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4484\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4480\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4506\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4634\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4790\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4823\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4393\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4465\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4646\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4312\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4451\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4493\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4248\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4467\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4313\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4254\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4369\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4164\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4254\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4184\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4107\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4176\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4054\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4070\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4084\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3969\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3997\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4003\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3890\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3881\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3914\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3839\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3757\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3726\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3740\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3801\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3824\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3954\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4529\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.5010\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4142\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4226\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4510\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3975\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4377\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3812\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4187\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3853\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.4111\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3812\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3940\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3871\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3802\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3833\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3680\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3779\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3639\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3701\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3571\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3608\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3540\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3528\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3465\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3469\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3423\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3400\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3348\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3347\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3283\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3285\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3210\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3231\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3169\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3130\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3204\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3034\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3073\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3114\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 2s 2ms/step - loss: 0.3023\n",
      " result={'acc': '0.773', 'prec': '0.712', 'rec': '0.483', 'f1': '0.560'}\n"
     ]
    }
   ],
   "source": [
    "clf_6th = HENIN(GCNXss_shape=multi_hot_users_6th.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all_6th[:,0,:].shape[1], \n",
    "            reg=l2(1e-4), n_layers=4, lr=0.01,\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN_6th, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN_6th, support=3)\n",
    "\n",
    "AvgCP_6th, AvgGP_6th = HENIN_cv(graph=graph_6th, y=y_all_6th, A=ppA_6th, model=clf_6th, epochs=40)\n",
    "#best_results[tuple(param)] = AvgGP\n",
    "print(f\" result={AvgGP_6th}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "patent-liberia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<249x7549 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7793 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_hot_users_6th"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liable-cisco",
   "metadata": {},
   "source": [
    "# Unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "operational-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load data\n",
    "'''\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4ModelVINE_COMBINED.pickle', 'rb') as f:\n",
    "    Dat4Model_cb = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_usersVINE_COMBINED.pickle', 'rb') as f:\n",
    "    multi_hot_users_cb = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all_cb = Dat4Model_cb['w2v_vec_all'] # features for HENIN\n",
    "y_all_cb = Dat4Model_cb['y_all'] # target for HENIN\n",
    "textFeat_all_cb = Dat4Model_cb['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN_cb = w2v_vec_all_cb.shape[2]\n",
    "MAX_REV_LEN_cb = w2v_vec_all_cb.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb_cb = pad_sequences(w2v_vec_all_cb[:,0,:,:], maxlen=MAX_REV_LEN_cb, dtype='float32', padding='post') \n",
    "\n",
    "ppA_cb = genAdjacencyMatrix(textFeat_all_cb[:,0,:], 'cosine')\n",
    "ssA_cb = genAdjacencyMatrix(multi_hot_users_cb, 'cosine')\n",
    "\n",
    "graph_ss_cb = genGCNgraph(ssA_cb, multi_hot_users_cb)\n",
    "graph_pp_cb = genGCNgraph(ppA_cb, textFeat_all_cb[:,0,:])\n",
    "\n",
    "graph_cb = [w2v_vec_all_cb]+[postEmb_cb]+graph_ss_cb+graph_pp_cb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reverse-cooling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 29051)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           464832      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4816        input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_9 (GraphConvo (None, 16)           784         dropout_3[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_13 (GraphConv (None, 16)           784         dropout_4[0][0]                  \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_10 (GraphConv (None, 16)           784         graph_convolution_9[0][0]        \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_14 (GraphConv (None, 16)           784         graph_convolution_13[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_4 (Self_Attenti (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 64)     63936       input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_11 (GraphConv (None, 16)           784         graph_convolution_10[0][0]       \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_15 (GraphConv (None, 16)           784         graph_convolution_14[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 64)           0           self__attention_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_2 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_12 (GraphConv (None, 8)            392         graph_convolution_11[0][0]       \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_16 (GraphConv (None, 8)            392         graph_convolution_15[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 208)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 co_att_layer_2[0][0]             \n",
      "                                                                 graph_convolution_12[0][0]       \n",
      "                                                                 graph_convolution_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            209         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 623,015\n",
      "Trainable params: 623,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "968/968 [==============================] - 7s 7ms/step - loss: 0.7107\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.6861\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.6647\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.6538\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.6108\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5965\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5880\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5808\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5890\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5815\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5782\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5705\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.5629\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5529\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5464\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.5366\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5324\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.5276\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5221\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5221\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5184\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5117\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5098\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5095\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5067\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5045\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.5034\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.5021\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4993\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4963\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4947\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4946\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4953\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4957\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4924\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4847\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4856\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.4859\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4801\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4770\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4592\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4589\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4664\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4624\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4526\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.4546\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4543\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4467\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4449\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4532\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4555\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4495\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4405\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4448\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4414\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4360\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4358\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4363\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4295\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4279\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4259\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4262\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4300\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4290\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4246\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4213\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4205\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4199\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4192\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4174\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4173\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4183\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4269\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4473\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4428\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4149\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4266\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4241\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4157\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4242\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.4275\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4335\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4198\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4251\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4206\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4221\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4185\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4195\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4180\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4128\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4179\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4107\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4130\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4100\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4081\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4113\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4064\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4058\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4071\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4039\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4035\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4033\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4003\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4013\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3981\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3981\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3968\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3952\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3935\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3922\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3911\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3890\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3880\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3875\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.3863\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3887\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3995\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4149\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4042\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4097\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4121\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4048\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4056\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4121\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4072\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4013\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 4s 5ms/step - loss: 0.4019\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3984\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3971\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3966\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3940\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3926\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3946\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3972\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3981\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3903\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3890\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3956\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3980\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3889\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3829\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3872\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3889\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3810\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3775\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3784\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3830\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3776\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3657\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3771\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3792\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3843\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3846\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3793\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3716\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3733\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3713\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3626\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3624\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3843\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.4117\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3987\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3952\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3873\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3926\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3962\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3921\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3900\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3876\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3853\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3848\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3798\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3806\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3791\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3739\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3709\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3676\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3697\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3649\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3597\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3622\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3564\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3598\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3616\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3601\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3554\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3483\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3587\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3591\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3997\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3817\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3453\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3460\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3438\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3371\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3305\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3341\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3289\n",
      "Epoch 1/1\n",
      "968/968 [==============================] - 5s 5ms/step - loss: 0.3305\n",
      " result={'acc': '0.794', 'prec': '0.701', 'rec': '0.593', 'f1': '0.642'}\n"
     ]
    }
   ],
   "source": [
    "clf_cb = HENIN(GCNXss_shape=multi_hot_users_cb.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all_cb[:,0,:].shape[1], \n",
    "            reg=l2(1e-4), n_layers=4, lr=0.01,\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN_cb, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN_cb, support=3)\n",
    "\n",
    "AvgCP_cb, AvgGP_cb = HENIN_cv(graph=graph_cb, y=y_all_cb, A=ppA_cb, model=clf_cb, epochs=40)\n",
    "#best_results[tuple(param)] = AvgGP\n",
    "print(f\" result={AvgGP_cb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
