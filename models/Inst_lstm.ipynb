{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "freelance-czech",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "from collections import defaultdict\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D, GRU, Bidirectional\n",
    "from keras.layers import GlobalMaxPooling1D, LSTM, Dropout, SimpleRNN, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import concatenate\n",
    "from keras import activations, initializers, constraints\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1,l2, l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "from layers import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "velvet-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Replace self-attention layers with LSTM layers\n",
    "def HENIN_LSTM(GCNXss_shape, GCNXpp_shape, reg=l2(5e-4), n_layers=2, n_head=8, size_per_head=8, MAX_REV_LEN=75, MAX_REV_WORD_LEN=10, support=3):\n",
    "    \n",
    "    '''\n",
    "    Comment Encoding\n",
    "    '''\n",
    "    \n",
    "    ''' Capture reviews context correlation'''\n",
    "    ## word-level encoding\n",
    "    word_input = Input(shape=(None, 300), dtype='float32')\n",
    "    #word_sa = Self_Attention(n_head, size_per_head)(word_input)\n",
    "    word_sa = LSTM(n_head*size_per_head, return_sequences=True)(word_input)\n",
    "    word_avg = GlobalAveragePooling1D()(word_sa)\n",
    "    wordEncoder = Model(word_input, word_avg)\n",
    "    \n",
    "    ## review-level encoding\n",
    "    content_input = Input(shape=(MAX_REV_LEN, MAX_REV_WORD_LEN, 300), dtype='float32')\n",
    "    content_word_encode = TimeDistributed(wordEncoder, name='word_seq_encoder')(content_input)\n",
    "    #content_sa = Self_Attention(n_head, size_per_head)(content_word_encode)\n",
    "    content_sa = LSTM(n_head*size_per_head, return_sequences=True)(content_word_encode)\n",
    "    contentSA_avg_pool = GlobalAveragePooling1D()(content_sa) # session embedding\n",
    "    \n",
    "    ''' Capture Post-Comment co-attention'''\n",
    "    post_words_input = Input(shape=(None, 300), dtype='float32')\n",
    "    post_lstm = Bidirectional(GRU(32, return_sequences=True))(post_words_input)\n",
    "    coAtt_vec = CoAttLayer(MAX_REV_LEN)([content_word_encode, post_lstm])\n",
    "    \n",
    "    '''\n",
    "    GCN\n",
    "    Session-Session Interaction Extractor\n",
    "    Adjacency: session-session\n",
    "    '''\n",
    "    G_ss = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(3)]\n",
    "    \n",
    "    X_ss = Input(shape=(GCNXss_shape,))\n",
    "    X_ss_emb = Dense(16, activation='relu')(X_ss)\n",
    "    \n",
    "    # Define GCN model architecture\n",
    "    H_ss = Dropout(0.2)(X_ss_emb)\n",
    "    for i in range(n_layers-1):\n",
    "        H_ss = GraphConvolution(16, support, activation='relu', kernel_regularizer=reg)([H_ss]+G_ss)\n",
    "        \n",
    "    H_ss = GraphConvolution(8, support, activation='softmax', kernel_regularizer=reg)([H_ss]+G_ss)\n",
    "    \n",
    "    '''\n",
    "    GCN\n",
    "    Post-Post Interaction Extractor\n",
    "    Adjacency: post-post\n",
    "    '''\n",
    "    G_pp = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(3)]\n",
    "    \n",
    "    X_pp = Input(shape=(GCNXpp_shape,))\n",
    "    X_pp_emb = Dense(16, activation='relu')(X_pp)\n",
    "    \n",
    "    # Define GCN model architecture\n",
    "    H_pp = Dropout(0.2)(X_pp_emb)\n",
    "    for i in range(n_layers-1):\n",
    "        H_pp = GraphConvolution(16, support, activation='relu', kernel_regularizer=reg)([H_pp]+G_pp)\n",
    "    H_pp = GraphConvolution(8, support, activation='softmax', kernel_regularizer=reg)([H_pp]+G_pp)\n",
    "     \n",
    "    '''\n",
    "    Concatenate Comment Encoding & GCN Embedding\n",
    "    '''\n",
    "    H = concatenate([contentSA_avg_pool, coAtt_vec, H_ss, H_pp])\n",
    "    Y = Dense(1, activation='sigmoid')(H)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs=[content_input]+[post_words_input]+[X_ss]+G_ss+[X_pp]+G_pp, outputs=Y)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01))\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "future-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load data\n",
    "'''\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4Model.pickle', 'rb') as f:\n",
    "    Dat4Model = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_users.pickle', 'rb') as f:\n",
    "    multi_hot_users = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all = Dat4Model['w2v_vec_all'] # features for HENIN\n",
    "y_all = Dat4Model['y_all'] # target for HENIN\n",
    "textFeat_all = Dat4Model['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN = w2v_vec_all.shape[2]\n",
    "MAX_REV_LEN = w2v_vec_all.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb = pad_sequences(w2v_vec_all[:,0,:,:], maxlen=MAX_REV_LEN, dtype='float32', padding='post') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "underlying-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross validating for HENIN model\n",
    "def HENIN_cv(graph, y, A, model, epochs):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=9999, shuffle=True)\n",
    "    iters = 0\n",
    "    \n",
    "    for train_index, test_index in skf.split(range(len(y)), y):\n",
    "        y_train, y_test, train_mask = Mask_y(y=y, train_ix=train_index, test_ix=test_index)\n",
    "        #y_train, y_test = Mask_y(y=y, train_ix=train_index, test_ix=test_index)\n",
    "        clf = model\n",
    "        for epoch in range(epochs):\n",
    "            # Changed sample weight, need to change back \n",
    "            clf.fit(graph, y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=1)\n",
    "        preds = (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)\n",
    "        \n",
    "        completePerform = metrics(y, preds) # Complete set performance\n",
    "        generalPerform = metrics(y[test_index], preds[test_index]) # test set performance\n",
    "        \n",
    "        try:\n",
    "            if iters == 1:\n",
    "                CP = {k: v + [completePerform[k]] for k, v in CP.items()}\n",
    "                GP = {k: v + [generalPerform[k]] for k, v in GP.items()}\n",
    "            else:  \n",
    "                CP = {k: [v] + [completePerform[k]] for k, v in CP.items()}\n",
    "                GP = {k: [v] + [generalPerform[k]] for k, v in GP.items()}\n",
    "                iters += 1\n",
    "        except:\n",
    "            CP = completePerform\n",
    "            GP = generalPerform\n",
    "    \n",
    "    AvgCP = {k: '{:.3f}'.format(np.mean(v)) for k, v in CP.items()}\n",
    "    AvgGP = {k: '{:.3f}'.format(np.mean(v)) for k, v in GP.items()}\n",
    "    \n",
    "    return AvgCP, AvgGP\n",
    "\n",
    "OurComResult = {}\n",
    "OurGenResult = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "employed-wayne",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer global_average_pooling1d_12: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-2e3c3f782e4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5e-4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mn_head\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_per_head\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_REV_LEN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_REV_LEN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mAvgCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAvgGP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHENIN_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mppA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-04ebcf70f53a>\u001b[0m in \u001b[0;36mHENIN\u001b[1;34m(GCNXss_shape, GCNXpp_shape, reg, n_layers, n_head, size_per_head, MAX_REV_LEN, MAX_REV_WORD_LEN, support)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#word_sa = Self_Attention(n_head, size_per_head)(word_input)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mword_sa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mword_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_sa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mwordEncoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_avg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[1;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[1;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 414\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    415\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m                 \u001b[1;31m# Collect input shapes to build layer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': expected ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', found ndim='\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m                                      str(K.ndim(x)))\n\u001b[0m\u001b[0;32m    312\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 is incompatible with layer global_average_pooling1d_12: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "## HENIN\n",
    "ppA = genAdjacencyMatrix(textFeat_all[:,0,:], 'cosine')\n",
    "ssA = genAdjacencyMatrix(multi_hot_users, 'cosine')\n",
    "\n",
    "graph_ss = genGCNgraph(ssA, multi_hot_users)\n",
    "graph_pp = genGCNgraph(ppA, textFeat_all[:,0,:])\n",
    "\n",
    "graph = [w2v_vec_all]+[postEmb]+graph_ss+graph_pp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "arranged-pressing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_70 (InputLayer)           (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_74 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16)           1154832     input_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 16)           4816        input_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16)           0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_67 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_68 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_69 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16)           0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_71 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_72 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_73 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_65 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_37 (GraphConv (None, 16)           784         dropout_11[0][0]                 \n",
      "                                                                 input_67[0][0]                   \n",
      "                                                                 input_68[0][0]                   \n",
      "                                                                 input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_41 (GraphConv (None, 16)           784         dropout_12[0][0]                 \n",
      "                                                                 input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "                                                                 input_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       93440       input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_66 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_38 (GraphConv (None, 16)           784         graph_convolution_37[0][0]       \n",
      "                                                                 input_67[0][0]                   \n",
      "                                                                 input_68[0][0]                   \n",
      "                                                                 input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_42 (GraphConv (None, 16)           784         graph_convolution_41[0][0]       \n",
      "                                                                 input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "                                                                 input_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 75, 64)       33024       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, None, 64)     63936       input_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_39 (GraphConv (None, 16)           784         graph_convolution_38[0][0]       \n",
      "                                                                 input_67[0][0]                   \n",
      "                                                                 input_68[0][0]                   \n",
      "                                                                 input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_43 (GraphConv (None, 16)           784         graph_convolution_42[0][0]       \n",
      "                                                                 input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "                                                                 input_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_19 (Gl (None, 64)           0           lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_6 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_40 (GraphConv (None, 8)            392         graph_convolution_39[0][0]       \n",
      "                                                                 input_67[0][0]                   \n",
      "                                                                 input_68[0][0]                   \n",
      "                                                                 input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_44 (GraphConv (None, 8)            392         graph_convolution_43[0][0]       \n",
      "                                                                 input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "                                                                 input_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 208)          0           global_average_pooling1d_19[0][0]\n",
      "                                                                 co_att_layer_6[0][0]             \n",
      "                                                                 graph_convolution_40[0][0]       \n",
      "                                                                 graph_convolution_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            209         concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,369,591\n",
      "Trainable params: 1,369,591\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 49s 22ms/step - loss: 0.7651\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 48s 22ms/step - loss: 0.7208\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 41s 19ms/step - loss: 0.6620\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 43s 19ms/step - loss: 0.6426\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 40s 18ms/step - loss: 0.5401\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 42s 19ms/step - loss: 0.6142\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 41s 19ms/step - loss: 0.5322\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 48s 22ms/step - loss: 0.5087\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 61s 28ms/step - loss: 0.5123\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 51s 23ms/step - loss: 0.4948\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 56s 26ms/step - loss: 0.4443\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 44s 20ms/step - loss: 0.4414\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 43s 19ms/step - loss: 0.4357\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 46s 21ms/step - loss: 0.4088\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 46s 21ms/step - loss: 0.4189\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 44s 20ms/step - loss: 0.4086\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 67s 30ms/step - loss: 0.3892\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 40s 18ms/step - loss: 0.4021\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 76s 35ms/step - loss: 0.3749\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.3809\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 42s 19ms/step - loss: 0.3715\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3600\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.3647\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3514\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.3526\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.3474\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 39s 18ms/step - loss: 0.3393\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3419\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3330\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3363\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 28s 13ms/step - loss: 0.3489\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 39s 18ms/step - loss: 0.3489\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 16ms/step - loss: 0.3448\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3435\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.3395\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.3376\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 40s 18ms/step - loss: 0.3358\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.3336\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3332\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 15ms/step - loss: 0.3305\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.3292\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3264\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.3247\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3226\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3205\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 17ms/step - loss: 0.3195\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 16ms/step - loss: 0.3185\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.3200\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.3262\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3201\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.3190\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.3215\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3112\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3146\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.3214\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 15ms/step - loss: 0.3072\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 15ms/step - loss: 0.3299\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3226\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.3194\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3168\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 26s 12ms/step - loss: 0.3126\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.3127\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.3118\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3125\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.3060\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3069\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 15ms/step - loss: 0.3020\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3068\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3042\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.2959\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 16ms/step - loss: 0.3022\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 32s 14ms/step - loss: 0.3035\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.2935\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.3038\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.2910\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2923\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2911\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 32s 14ms/step - loss: 0.2867\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2802\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.2893\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.2972\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2768\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 41s 18ms/step - loss: 0.2958\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 39s 18ms/step - loss: 0.2866\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 39s 17ms/step - loss: 0.2819\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 40s 18ms/step - loss: 0.2888\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 40s 18ms/step - loss: 0.2669\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.2748\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 39s 18ms/step - loss: 0.2675\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 39s 18ms/step - loss: 0.2587\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 25s 11ms/step - loss: 0.2856\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.2837\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 40s 18ms/step - loss: 0.2783\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.2729\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.2769\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.2678\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.2625\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 41s 18ms/step - loss: 0.2665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 40s 18ms/step - loss: 0.2704\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 39s 18ms/step - loss: 0.2684\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.2782\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 42s 19ms/step - loss: 0.2501\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 17ms/step - loss: 0.2578\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.2718\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 41s 19ms/step - loss: 0.2411\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 43s 19ms/step - loss: 0.2798\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.2891\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.2463\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 31s 14ms/step - loss: 0.3490\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2478\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 29s 13ms/step - loss: 0.2715\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 32s 15ms/step - loss: 0.2416\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.2594\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2679\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2574\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2489\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.2539\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2445\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.2449\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.2392\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 23s 10ms/step - loss: 0.2399\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 32s 15ms/step - loss: 0.2629\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.2343\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.2668\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 31s 14ms/step - loss: 0.2307\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 31s 14ms/step - loss: 0.2515\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 32s 15ms/step - loss: 0.2241\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 30s 14ms/step - loss: 0.2408\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.2194\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 30s 14ms/step - loss: 0.2276\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2166\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 15ms/step - loss: 0.2173\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2137\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 32s 15ms/step - loss: 0.2082\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.2050\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 32s 14ms/step - loss: 0.1966\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 34s 15ms/step - loss: 0.1949\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.1941\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.1883\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.1830\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.1854\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 43s 19ms/step - loss: 0.2338\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 39s 17ms/step - loss: 0.2260\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 38s 17ms/step - loss: 0.1997\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.2094\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.1958\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.1982\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.1836\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 0.1887\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 36s 16ms/step - loss: 0.1756\n",
      "{'acc': '0.873', 'prec': '0.807', 'rec': '0.771', 'f1': '0.787'}\n"
     ]
    }
   ],
   "source": [
    "clf = HENIN_LSTM(GCNXss_shape=multi_hot_users.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all[:,0,:].shape[1], \n",
    "            reg=l2(5e-4), n_layers=4,\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n",
    "\n",
    "AvgCP, AvgGP = HENIN_cv(graph=graph, y=y_all, A=ppA, model=clf, epochs=40)\n",
    "print(AvgGP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "lasting-railway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "lstm_layer = clf.to_json()\n",
    "with open(\"lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(lstm_layer)\n",
    "# serialize weights to HDF5\n",
    "clf.save_weights(\"lstm_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
