{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-recorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "from collections import defaultdict\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D, GRU, Bidirectional\n",
    "from keras.layers import GlobalMaxPooling1D, LSTM, Dropout, SimpleRNN, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import concatenate\n",
    "from keras import activations, initializers, constraints\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1,l2, l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "from layers import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latest-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The proposed model, HENIN\n",
    "def HENIN(GCNXss_shape, GCNXpp_shape, reg=l2(5e-4), n_layers=2, lr=0.01, n_head=8, size_per_head=8, MAX_REV_LEN=75, MAX_REV_WORD_LEN=10, support=3):\n",
    "    \n",
    "    '''\n",
    "    Comment Encoding\n",
    "    '''\n",
    "    \n",
    "    ''' Capture reviews context correlation'''\n",
    "    ## word-level encoding\n",
    "    word_input = Input(shape=(None, 300), dtype='float32')\n",
    "    word_sa = Self_Attention(n_head, size_per_head)(word_input)\n",
    "    word_avg = GlobalAveragePooling1D()(word_sa)\n",
    "    wordEncoder = Model(word_input, word_avg)\n",
    "    \n",
    "    ## review-level encoding\n",
    "    content_input = Input(shape=(MAX_REV_LEN, MAX_REV_WORD_LEN, 300), dtype='float32')\n",
    "    content_word_encode = TimeDistributed(wordEncoder, name='word_seq_encoder')(content_input)\n",
    "    content_sa = Self_Attention(n_head, size_per_head)(content_word_encode)\n",
    "    contentSA_avg_pool = GlobalAveragePooling1D()(content_sa) # session embedding\n",
    "    \n",
    "    ''' Capture Post-Comment co-attention'''\n",
    "    post_words_input = Input(shape=(None, 300), dtype='float32')\n",
    "    post_lstm = Bidirectional(GRU(32, return_sequences=True))(post_words_input)\n",
    "    coAtt_vec = CoAttLayer(MAX_REV_LEN)([content_word_encode, post_lstm])\n",
    "    \n",
    "    '''\n",
    "    GCN\n",
    "    Session-Session Interaction Extractor\n",
    "    Adjacency: session-session\n",
    "    '''\n",
    "    G_ss = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(3)]\n",
    "    \n",
    "    X_ss = Input(shape=(GCNXss_shape,))\n",
    "    X_ss_emb = Dense(16, activation='relu')(X_ss)\n",
    "    \n",
    "    # Define GCN model architecture\n",
    "    H_ss = Dropout(0.2)(X_ss_emb)\n",
    "    for i in range(n_layers-1):\n",
    "        H_ss = GraphConvolution(16, support, activation='relu', kernel_regularizer=reg)([H_ss]+G_ss)\n",
    "        \n",
    "    H_ss = GraphConvolution(8, support, activation='softmax', kernel_regularizer=reg)([H_ss]+G_ss)\n",
    "    \n",
    "    '''\n",
    "    GCN\n",
    "    Post-Post Interaction Extractor\n",
    "    Adjacency: post-post\n",
    "    '''\n",
    "    G_pp = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(3)]\n",
    "    \n",
    "    X_pp = Input(shape=(GCNXpp_shape,))\n",
    "    X_pp_emb = Dense(16, activation='relu')(X_pp)\n",
    "    \n",
    "    # Define GCN model architecture\n",
    "    H_pp = Dropout(0.2)(X_pp_emb)\n",
    "    for i in range(n_layers-1):\n",
    "        H_pp = GraphConvolution(16, support, activation='relu', kernel_regularizer=reg)([H_pp]+G_pp)\n",
    "    H_pp = GraphConvolution(8, support, activation='softmax', kernel_regularizer=reg)([H_pp]+G_pp)\n",
    "     \n",
    "    '''\n",
    "    Concatenate Comment Encoding & GCN Embedding\n",
    "    '''\n",
    "    H = concatenate([contentSA_avg_pool, coAtt_vec, H_ss, H_pp])\n",
    "    Y = Dense(1, activation='sigmoid')(H)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs=[content_input]+[post_words_input]+[X_ss]+G_ss+[X_pp]+G_pp, outputs=Y)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr))\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acceptable-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load data\n",
    "'''\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4Model.pickle', 'rb') as f:\n",
    "    Dat4Model = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_users.pickle', 'rb') as f:\n",
    "    multi_hot_users = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all = Dat4Model['w2v_vec_all'] # features for HENIN\n",
    "y_all = Dat4Model['y_all'] # target for HENIN\n",
    "textFeat_all = Dat4Model['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN = w2v_vec_all.shape[2]\n",
    "MAX_REV_LEN = w2v_vec_all.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb = pad_sequences(w2v_vec_all[:,0,:,:], maxlen=MAX_REV_LEN, dtype='float32', padding='post') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "governmental-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross validating for HENIN model\n",
    "def HENIN_cv(graph, y, A, model, epochs):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=9999, shuffle=True)\n",
    "    iters = 0\n",
    "    \n",
    "    for train_index, test_index in skf.split(range(len(y)), y):\n",
    "        y_train, y_test, train_mask = Mask_y(y=y, train_ix=train_index, test_ix=test_index)\n",
    "        #y_train, y_test = Mask_y(y=y, train_ix=train_index, test_ix=test_index)\n",
    "        clf = model\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            clf.fit(graph, y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=1)\n",
    "            #if epoch%5==0:\n",
    "                #print(metrics(y[test_index], (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)[test_index]))\n",
    "        preds = (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)\n",
    "        \n",
    "        completePerform = metrics(y, preds) # Complete set performance\n",
    "        generalPerform = metrics(y[test_index], preds[test_index]) # test set performance\n",
    "        \n",
    "          \n",
    "        try:\n",
    "            if iters == 1:\n",
    "                CP = {k: v + [completePerform[k]] for k, v in CP.items()}\n",
    "                GP = {k: v + [generalPerform[k]] for k, v in GP.items()}\n",
    "            else:  \n",
    "                CP = {k: [v] + [completePerform[k]] for k, v in CP.items()}\n",
    "                GP = {k: [v] + [generalPerform[k]] for k, v in GP.items()}\n",
    "                iters += 1\n",
    "        except:\n",
    "            CP = completePerform\n",
    "            GP = generalPerform\n",
    "    \n",
    "    AvgCP = {k: '{:.3f}'.format(np.mean(v)) for k, v in CP.items()}\n",
    "    AvgGP = {k: '{:.3f}'.format(np.mean(v)) for k, v in GP.items()}\n",
    "    \n",
    "    return AvgCP, AvgGP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "substantial-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-params tuning\n",
    " \n",
    "import time\n",
    "\n",
    "ppA = genAdjacencyMatrix(textFeat_all[:,0,:], 'cosine')\n",
    "ssA = genAdjacencyMatrix(multi_hot_users, 'cosine')\n",
    "\n",
    "graph_ss = genGCNgraph(ssA, multi_hot_users)\n",
    "graph_pp = genGCNgraph(ppA, textFeat_all[:,0,:])\n",
    "\n",
    "graph = [w2v_vec_all]+[postEmb]+graph_ss+graph_pp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "recreational-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_51 (InputLayer)           (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_55 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 16)           1154832     input_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 16)           4816        input_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 16)           0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_49 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_50 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 16)           0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_52 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_53 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_54 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_46 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_33 (GraphConv (None, 16)           784         dropout_9[0][0]                  \n",
      "                                                                 input_48[0][0]                   \n",
      "                                                                 input_49[0][0]                   \n",
      "                                                                 input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_37 (GraphConv (None, 16)           784         dropout_10[0][0]                 \n",
      "                                                                 input_52[0][0]                   \n",
      "                                                                 input_53[0][0]                   \n",
      "                                                                 input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_47 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_34 (GraphConv (None, 16)           784         graph_convolution_33[0][0]       \n",
      "                                                                 input_48[0][0]                   \n",
      "                                                                 input_49[0][0]                   \n",
      "                                                                 input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_38 (GraphConv (None, 16)           784         graph_convolution_37[0][0]       \n",
      "                                                                 input_52[0][0]                   \n",
      "                                                                 input_53[0][0]                   \n",
      "                                                                 input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_10 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, None, 64)     63936       input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_35 (GraphConv (None, 16)           784         graph_convolution_34[0][0]       \n",
      "                                                                 input_48[0][0]                   \n",
      "                                                                 input_49[0][0]                   \n",
      "                                                                 input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_39 (GraphConv (None, 16)           784         graph_convolution_38[0][0]       \n",
      "                                                                 input_52[0][0]                   \n",
      "                                                                 input_53[0][0]                   \n",
      "                                                                 input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 64)           0           self__attention_10[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_5 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_36 (GraphConv (None, 8)            392         graph_convolution_35[0][0]       \n",
      "                                                                 input_48[0][0]                   \n",
      "                                                                 input_49[0][0]                   \n",
      "                                                                 input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_40 (GraphConv (None, 8)            392         graph_convolution_39[0][0]       \n",
      "                                                                 input_52[0][0]                   \n",
      "                                                                 input_53[0][0]                   \n",
      "                                                                 input_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 208)          0           global_average_pooling1d_10[0][0]\n",
      "                                                                 co_att_layer_5[0][0]             \n",
      "                                                                 graph_convolution_36[0][0]       \n",
      "                                                                 graph_convolution_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            209         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,313,015\n",
      "Trainable params: 1,313,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6952\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6934\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.6908\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6883\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.6858\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6830\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6802\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6772\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6740\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.6708\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6673\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.6636\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6597\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6556\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6512\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6480\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6434\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.6385\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6333\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.6278\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6221\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6158\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6090\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6016\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5940\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5862\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5788\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5713\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5635\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5552\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5428\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5333\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5235\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5140\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5051\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4978\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4896\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4807\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4733\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.4665\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.4595\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.4518\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.4444\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.4404\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.4321\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4301\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4247\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4176\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4197\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4099\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4103\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4067\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4022\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4017\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3973\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3947\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3946\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3921\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3914\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3893\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3887\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3872\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3842\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3831\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3819\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3799\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3787\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3762\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3747\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3728\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3709\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3690\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3669\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3646\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.3626\n",
      "i=4,l={'l1': 0.0, 'l2': 9.999999747378752e-05}, lr=0.001, {'acc': '0.805', 'prec': '0.846', 'rec': '0.476', 'f1': '0.549'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_62 (InputLayer)           (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_66 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 16)           1154832     input_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 16)           4816        input_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 16)           0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_59 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_60 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_61 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 16)           0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_63 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_64 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_65 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_57 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_41 (GraphConv (None, 16)           784         dropout_11[0][0]                 \n",
      "                                                                 input_59[0][0]                   \n",
      "                                                                 input_60[0][0]                   \n",
      "                                                                 input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_45 (GraphConv (None, 16)           784         dropout_12[0][0]                 \n",
      "                                                                 input_63[0][0]                   \n",
      "                                                                 input_64[0][0]                   \n",
      "                                                                 input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_58 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_42 (GraphConv (None, 16)           784         graph_convolution_41[0][0]       \n",
      "                                                                 input_59[0][0]                   \n",
      "                                                                 input_60[0][0]                   \n",
      "                                                                 input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_46 (GraphConv (None, 16)           784         graph_convolution_45[0][0]       \n",
      "                                                                 input_63[0][0]                   \n",
      "                                                                 input_64[0][0]                   \n",
      "                                                                 input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_12 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, None, 64)     63936       input_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_43 (GraphConv (None, 16)           784         graph_convolution_42[0][0]       \n",
      "                                                                 input_59[0][0]                   \n",
      "                                                                 input_60[0][0]                   \n",
      "                                                                 input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_47 (GraphConv (None, 16)           784         graph_convolution_46[0][0]       \n",
      "                                                                 input_63[0][0]                   \n",
      "                                                                 input_64[0][0]                   \n",
      "                                                                 input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 64)           0           self__attention_12[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_6 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_44 (GraphConv (None, 8)            392         graph_convolution_43[0][0]       \n",
      "                                                                 input_59[0][0]                   \n",
      "                                                                 input_60[0][0]                   \n",
      "                                                                 input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_48 (GraphConv (None, 8)            392         graph_convolution_47[0][0]       \n",
      "                                                                 input_63[0][0]                   \n",
      "                                                                 input_64[0][0]                   \n",
      "                                                                 input_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 208)          0           global_average_pooling1d_12[0][0]\n",
      "                                                                 co_att_layer_6[0][0]             \n",
      "                                                                 graph_convolution_44[0][0]       \n",
      "                                                                 graph_convolution_48[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            209         concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,313,015\n",
      "Trainable params: 1,313,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 22s 10ms/step - loss: 0.6987\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6903\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6814\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6714\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6603\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6480\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6340\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6183\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6000\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5784\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5564\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.5376\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5191\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4990\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4798\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4755\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4659\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4566\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4573\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4489\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4418\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4417\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4332\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4276\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4247\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4149\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4106\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4078\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4014\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3954\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3983\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3960\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3975\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3960\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3957\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3938\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3898\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3870\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.3852\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3852\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3828\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3809\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3798\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3784\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3765\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3792\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3774\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3756\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3739\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3722\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3711\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3700\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3687\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3675\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3660\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3646\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3630\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3614\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3598\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3586\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3544\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3530\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3514\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3496\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3481\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3464\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3450\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3434\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3418\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3403\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3386\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3371\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3358\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3346\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3328\n",
      "i=4,l={'l1': 0.0, 'l2': 9.999999747378752e-05}, lr=0.003, {'acc': '0.831', 'prec': '0.821', 'rec': '0.567', 'f1': '0.666'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_73 (InputLayer)           (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_77 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 16)           1154832     input_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 16)           4816        input_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 16)           0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_70 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_71 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_72 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 16)           0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_74 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_75 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_76 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_68 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_49 (GraphConv (None, 16)           784         dropout_13[0][0]                 \n",
      "                                                                 input_70[0][0]                   \n",
      "                                                                 input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_53 (GraphConv (None, 16)           784         dropout_14[0][0]                 \n",
      "                                                                 input_74[0][0]                   \n",
      "                                                                 input_75[0][0]                   \n",
      "                                                                 input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_69 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_50 (GraphConv (None, 16)           784         graph_convolution_49[0][0]       \n",
      "                                                                 input_70[0][0]                   \n",
      "                                                                 input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_54 (GraphConv (None, 16)           784         graph_convolution_53[0][0]       \n",
      "                                                                 input_74[0][0]                   \n",
      "                                                                 input_75[0][0]                   \n",
      "                                                                 input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_14 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, None, 64)     63936       input_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_51 (GraphConv (None, 16)           784         graph_convolution_50[0][0]       \n",
      "                                                                 input_70[0][0]                   \n",
      "                                                                 input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_55 (GraphConv (None, 16)           784         graph_convolution_54[0][0]       \n",
      "                                                                 input_74[0][0]                   \n",
      "                                                                 input_75[0][0]                   \n",
      "                                                                 input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_14 (Gl (None, 64)           0           self__attention_14[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_7 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_52 (GraphConv (None, 8)            392         graph_convolution_51[0][0]       \n",
      "                                                                 input_70[0][0]                   \n",
      "                                                                 input_71[0][0]                   \n",
      "                                                                 input_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_56 (GraphConv (None, 8)            392         graph_convolution_55[0][0]       \n",
      "                                                                 input_74[0][0]                   \n",
      "                                                                 input_75[0][0]                   \n",
      "                                                                 input_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 208)          0           global_average_pooling1d_14[0][0]\n",
      "                                                                 co_att_layer_7[0][0]             \n",
      "                                                                 graph_convolution_52[0][0]       \n",
      "                                                                 graph_convolution_56[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1)            209         concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,313,015\n",
      "Trainable params: 1,313,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 23s 10ms/step - loss: 0.7040\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6846\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6541\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6183\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5615\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5119\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4788\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4448\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.5218\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4403\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4420\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4485\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4067\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4016\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4091\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4187\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3968\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3964\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4038\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3998\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3876\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3819\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3859\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3872\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3792\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3726\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3731\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3741\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3697\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3641\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3583\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3605\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3586\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3525\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3512\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3518\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3466\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3418\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3416\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3405\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3425\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3390\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3379\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3332\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3316\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3398\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3387\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3381\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3378\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3360\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3339\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3320\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3312\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3302\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3298\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3289\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3274\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3258\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3236\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3230\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3141\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3123\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3116\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3110\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3098\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3095\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3080\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3072\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3048\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3039\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3017\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3006\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.2984\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.2979\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.2958\n",
      "i=4,l={'l1': 0.0, 'l2': 9.999999747378752e-05}, lr=0.01, {'acc': '0.849', 'prec': '0.789', 'rec': '0.692', 'f1': '0.737'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_84 (InputLayer)           (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_88 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 16)           1154832     input_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 16)           4816        input_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 16)           0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_81 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_82 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_83 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 16)           0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_85 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_86 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_87 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_57 (GraphConv (None, 16)           784         dropout_15[0][0]                 \n",
      "                                                                 input_81[0][0]                   \n",
      "                                                                 input_82[0][0]                   \n",
      "                                                                 input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_62 (GraphConv (None, 16)           784         dropout_16[0][0]                 \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "                                                                 input_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_79 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_58 (GraphConv (None, 16)           784         graph_convolution_57[0][0]       \n",
      "                                                                 input_81[0][0]                   \n",
      "                                                                 input_82[0][0]                   \n",
      "                                                                 input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_63 (GraphConv (None, 16)           784         graph_convolution_62[0][0]       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "                                                                 input_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_80 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_59 (GraphConv (None, 16)           784         graph_convolution_58[0][0]       \n",
      "                                                                 input_81[0][0]                   \n",
      "                                                                 input_82[0][0]                   \n",
      "                                                                 input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_64 (GraphConv (None, 16)           784         graph_convolution_63[0][0]       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "                                                                 input_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_16 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, None, 64)     63936       input_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_60 (GraphConv (None, 16)           784         graph_convolution_59[0][0]       \n",
      "                                                                 input_81[0][0]                   \n",
      "                                                                 input_82[0][0]                   \n",
      "                                                                 input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_65 (GraphConv (None, 16)           784         graph_convolution_64[0][0]       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "                                                                 input_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 64)           0           self__attention_16[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_8 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_61 (GraphConv (None, 8)            392         graph_convolution_60[0][0]       \n",
      "                                                                 input_81[0][0]                   \n",
      "                                                                 input_82[0][0]                   \n",
      "                                                                 input_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_66 (GraphConv (None, 8)            392         graph_convolution_65[0][0]       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "                                                                 input_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 208)          0           global_average_pooling1d_16[0][0]\n",
      "                                                                 co_att_layer_8[0][0]             \n",
      "                                                                 graph_convolution_61[0][0]       \n",
      "                                                                 graph_convolution_66[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1)            209         concatenate_8[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,314,583\n",
      "Trainable params: 1,314,583\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 27s 12ms/step - loss: 1.0040\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 1.0006\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9961\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9919\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9876\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9831\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9784\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9736\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9687\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9635\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.9582\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9528\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9472\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9413\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9353\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9301\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9237\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.9171\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9103\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9032\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8957\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8879\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8795\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.8706\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8610\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.8509\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.8405\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.8303\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8200\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.8095\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.7959\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.7840\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.7716\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.7590\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7466\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7353\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7255\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7143\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7033\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6937\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6845\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6747\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6650\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6580\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6484\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6428\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6357\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6280\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6248\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.6189\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6149\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.6077\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6044\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5986\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5950\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5899\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5870\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.5835\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5809\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5789\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5771\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5741\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.5710\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5678\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5648\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5617\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5586\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5556\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5522\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5490\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5456\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5421\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.5388\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5354\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5321\n",
      "i=5,l={'l1': 0.0003000000142492354, 'l2': 0.0}, lr=0.001, {'acc': '0.806', 'prec': '0.824', 'rec': '0.484', 'f1': '0.566'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_95 (InputLayer)           (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_99 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 16)           1154832     input_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 16)           4816        input_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 16)           0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_92 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_93 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_94 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 16)           0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_96 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_97 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_98 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_67 (GraphConv (None, 16)           784         dropout_17[0][0]                 \n",
      "                                                                 input_92[0][0]                   \n",
      "                                                                 input_93[0][0]                   \n",
      "                                                                 input_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_72 (GraphConv (None, 16)           784         dropout_18[0][0]                 \n",
      "                                                                 input_96[0][0]                   \n",
      "                                                                 input_97[0][0]                   \n",
      "                                                                 input_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_90 (InputLayer)           (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_68 (GraphConv (None, 16)           784         graph_convolution_67[0][0]       \n",
      "                                                                 input_92[0][0]                   \n",
      "                                                                 input_93[0][0]                   \n",
      "                                                                 input_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_73 (GraphConv (None, 16)           784         graph_convolution_72[0][0]       \n",
      "                                                                 input_96[0][0]                   \n",
      "                                                                 input_97[0][0]                   \n",
      "                                                                 input_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_91 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_69 (GraphConv (None, 16)           784         graph_convolution_68[0][0]       \n",
      "                                                                 input_92[0][0]                   \n",
      "                                                                 input_93[0][0]                   \n",
      "                                                                 input_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_74 (GraphConv (None, 16)           784         graph_convolution_73[0][0]       \n",
      "                                                                 input_96[0][0]                   \n",
      "                                                                 input_97[0][0]                   \n",
      "                                                                 input_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_18 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, None, 64)     63936       input_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_70 (GraphConv (None, 16)           784         graph_convolution_69[0][0]       \n",
      "                                                                 input_92[0][0]                   \n",
      "                                                                 input_93[0][0]                   \n",
      "                                                                 input_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_75 (GraphConv (None, 16)           784         graph_convolution_74[0][0]       \n",
      "                                                                 input_96[0][0]                   \n",
      "                                                                 input_97[0][0]                   \n",
      "                                                                 input_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_18 (Gl (None, 64)           0           self__attention_18[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_9 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_71 (GraphConv (None, 8)            392         graph_convolution_70[0][0]       \n",
      "                                                                 input_92[0][0]                   \n",
      "                                                                 input_93[0][0]                   \n",
      "                                                                 input_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_76 (GraphConv (None, 8)            392         graph_convolution_75[0][0]       \n",
      "                                                                 input_96[0][0]                   \n",
      "                                                                 input_97[0][0]                   \n",
      "                                                                 input_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 208)          0           global_average_pooling1d_18[0][0]\n",
      "                                                                 co_att_layer_9[0][0]             \n",
      "                                                                 graph_convolution_71[0][0]       \n",
      "                                                                 graph_convolution_76[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 1)            209         concatenate_9[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,314,583\n",
      "Trainable params: 1,314,583\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 26s 12ms/step - loss: 1.0041\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9902\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9759\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9610\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9452\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9283\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9100\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8900\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8673\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8410\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8134\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7870\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7598\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7316\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7067\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6972\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6839\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6615\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6534\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6443\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 22s 10ms/step - loss: 0.6325\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6265\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.6112\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6049\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5869\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5800\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5704\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.5630\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 23s 10ms/step - loss: 0.5541\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5451\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5310\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5227\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5147\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5064\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4991\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4919\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4861\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4805\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4750\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 22s 10ms/step - loss: 0.4687\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4629\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4566\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4510\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4446\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.4391\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4426\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4371\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4324\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4277\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4235\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4200\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4155\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4110\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4073\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4028\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3977\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3944\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3904\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3856\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3828\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3732\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3689\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3645\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3611\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3569\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3536\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3498\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3458\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3422\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3388\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3353\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 23s 10ms/step - loss: 0.3318\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3285\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3252\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3219\n",
      "i=5,l={'l1': 0.0003000000142492354, 'l2': 0.0}, lr=0.003, {'acc': '0.839', 'prec': '0.804', 'rec': '0.617', 'f1': '0.691'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_106 (InputLayer)          (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_110 (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 16)           1154832     input_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 16)           4816        input_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 16)           0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_103 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_104 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_105 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 16)           0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_107 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_108 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_109 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_77 (GraphConv (None, 16)           784         dropout_19[0][0]                 \n",
      "                                                                 input_103[0][0]                  \n",
      "                                                                 input_104[0][0]                  \n",
      "                                                                 input_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_82 (GraphConv (None, 16)           784         dropout_20[0][0]                 \n",
      "                                                                 input_107[0][0]                  \n",
      "                                                                 input_108[0][0]                  \n",
      "                                                                 input_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_101 (InputLayer)          (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_78 (GraphConv (None, 16)           784         graph_convolution_77[0][0]       \n",
      "                                                                 input_103[0][0]                  \n",
      "                                                                 input_104[0][0]                  \n",
      "                                                                 input_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_83 (GraphConv (None, 16)           784         graph_convolution_82[0][0]       \n",
      "                                                                 input_107[0][0]                  \n",
      "                                                                 input_108[0][0]                  \n",
      "                                                                 input_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_102 (InputLayer)          (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_79 (GraphConv (None, 16)           784         graph_convolution_78[0][0]       \n",
      "                                                                 input_103[0][0]                  \n",
      "                                                                 input_104[0][0]                  \n",
      "                                                                 input_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_84 (GraphConv (None, 16)           784         graph_convolution_83[0][0]       \n",
      "                                                                 input_107[0][0]                  \n",
      "                                                                 input_108[0][0]                  \n",
      "                                                                 input_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_20 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, None, 64)     63936       input_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_80 (GraphConv (None, 16)           784         graph_convolution_79[0][0]       \n",
      "                                                                 input_103[0][0]                  \n",
      "                                                                 input_104[0][0]                  \n",
      "                                                                 input_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_85 (GraphConv (None, 16)           784         graph_convolution_84[0][0]       \n",
      "                                                                 input_107[0][0]                  \n",
      "                                                                 input_108[0][0]                  \n",
      "                                                                 input_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_20 (Gl (None, 64)           0           self__attention_20[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_10 (CoAttLayer)    (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_81 (GraphConv (None, 8)            392         graph_convolution_80[0][0]       \n",
      "                                                                 input_103[0][0]                  \n",
      "                                                                 input_104[0][0]                  \n",
      "                                                                 input_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_86 (GraphConv (None, 8)            392         graph_convolution_85[0][0]       \n",
      "                                                                 input_107[0][0]                  \n",
      "                                                                 input_108[0][0]                  \n",
      "                                                                 input_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 208)          0           global_average_pooling1d_20[0][0]\n",
      "                                                                 co_att_layer_10[0][0]            \n",
      "                                                                 graph_convolution_81[0][0]       \n",
      "                                                                 graph_convolution_86[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 1)            209         concatenate_10[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,314,583\n",
      "Trainable params: 1,314,583\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 37s 17ms/step - loss: 1.0351\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9859\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.9346\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.8791\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 23s 10ms/step - loss: 0.8157\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.7410\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6821\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6334\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6049\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6048\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5636\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5405\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5220\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5048\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4822\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4858\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4696\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4594\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4514\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4410\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4327\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4251\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4145\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4058\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3979\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3904\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3832\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3785\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.3738\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3693\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3624\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3612\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3604\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3575\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3549\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3522\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3491\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3468\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.3457\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3392\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3368\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3338\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.3332\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3309\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.3297\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3383\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3375\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3354\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3336\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.3333\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 22s 10ms/step - loss: 0.3297\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3291\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.3270\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.3254\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3245\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 22s 10ms/step - loss: 0.3212\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3197\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3177\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3171\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3164\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3106\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3093\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3076\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3051\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3036\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3043\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 21s 9ms/step - loss: 0.3002\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.3000\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.2953\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.2942\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.2912\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.2893\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 22s 10ms/step - loss: 0.2871\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.2846\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.2828\n",
      "i=5,l={'l1': 0.0003000000142492354, 'l2': 0.0}, lr=0.01, {'acc': '0.849', 'prec': '0.802', 'rec': '0.669', 'f1': '0.725'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_117 (InputLayer)          (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_121 (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 16)           1154832     input_117[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 16)           4816        input_121[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 16)           0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_114 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_115 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_116 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 16)           0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_118 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_119 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_120 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_112 (InputLayer)          (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_87 (GraphConv (None, 16)           784         dropout_21[0][0]                 \n",
      "                                                                 input_114[0][0]                  \n",
      "                                                                 input_115[0][0]                  \n",
      "                                                                 input_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_91 (GraphConv (None, 16)           784         dropout_22[0][0]                 \n",
      "                                                                 input_118[0][0]                  \n",
      "                                                                 input_119[0][0]                  \n",
      "                                                                 input_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_113 (InputLayer)          (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_88 (GraphConv (None, 16)           784         graph_convolution_87[0][0]       \n",
      "                                                                 input_114[0][0]                  \n",
      "                                                                 input_115[0][0]                  \n",
      "                                                                 input_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_92 (GraphConv (None, 16)           784         graph_convolution_91[0][0]       \n",
      "                                                                 input_118[0][0]                  \n",
      "                                                                 input_119[0][0]                  \n",
      "                                                                 input_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_22 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, None, 64)     63936       input_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_89 (GraphConv (None, 16)           784         graph_convolution_88[0][0]       \n",
      "                                                                 input_114[0][0]                  \n",
      "                                                                 input_115[0][0]                  \n",
      "                                                                 input_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_93 (GraphConv (None, 16)           784         graph_convolution_92[0][0]       \n",
      "                                                                 input_118[0][0]                  \n",
      "                                                                 input_119[0][0]                  \n",
      "                                                                 input_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_22 (Gl (None, 64)           0           self__attention_22[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_11 (CoAttLayer)    (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_90 (GraphConv (None, 8)            392         graph_convolution_89[0][0]       \n",
      "                                                                 input_114[0][0]                  \n",
      "                                                                 input_115[0][0]                  \n",
      "                                                                 input_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_94 (GraphConv (None, 8)            392         graph_convolution_93[0][0]       \n",
      "                                                                 input_118[0][0]                  \n",
      "                                                                 input_119[0][0]                  \n",
      "                                                                 input_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 208)          0           global_average_pooling1d_22[0][0]\n",
      "                                                                 co_att_layer_11[0][0]            \n",
      "                                                                 graph_convolution_90[0][0]       \n",
      "                                                                 graph_convolution_94[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1)            209         concatenate_11[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,313,015\n",
      "Trainable params: 1,313,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 35s 16ms/step - loss: 0.7277\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7247\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7217\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7187\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7157\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7125\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7093\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7058\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7022\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.6985\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.6945\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6904\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6862\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6816\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6770\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6735\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6686\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6633\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.6577\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6518\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6454\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6383\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6307\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.6226\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6144\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.6063\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5980\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5894\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5804\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5706\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5569\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5467\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.5368\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5280\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.5203\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5119\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5033\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4956\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4886\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4815\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4739\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4663\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4641\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4547\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4532\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4511\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4418\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4477\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4341\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4350\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4344\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4295\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4256\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4263\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4215\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4172\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4169\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4139\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4114\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4116\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4138\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4110\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4099\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4052\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4029\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4019\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3979\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3967\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3948\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3914\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3901\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3866\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3842\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3818\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3790\n",
      "i=4,l={'l1': 0.0, 'l2': 0.0003000000142492354}, lr=0.001, {'acc': '0.802', 'prec': '0.828', 'rec': '0.464', 'f1': '0.550'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_128 (InputLayer)          (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_132 (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 16)           1154832     input_128[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 16)           4816        input_132[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 16)           0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_125 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_126 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_127 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 16)           0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_129 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_130 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_131 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_123 (InputLayer)          (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_95 (GraphConv (None, 16)           784         dropout_23[0][0]                 \n",
      "                                                                 input_125[0][0]                  \n",
      "                                                                 input_126[0][0]                  \n",
      "                                                                 input_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_99 (GraphConv (None, 16)           784         dropout_24[0][0]                 \n",
      "                                                                 input_129[0][0]                  \n",
      "                                                                 input_130[0][0]                  \n",
      "                                                                 input_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_123[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_124 (InputLayer)          (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_96 (GraphConv (None, 16)           784         graph_convolution_95[0][0]       \n",
      "                                                                 input_125[0][0]                  \n",
      "                                                                 input_126[0][0]                  \n",
      "                                                                 input_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_100 (GraphCon (None, 16)           784         graph_convolution_99[0][0]       \n",
      "                                                                 input_129[0][0]                  \n",
      "                                                                 input_130[0][0]                  \n",
      "                                                                 input_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_24 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, None, 64)     63936       input_124[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_97 (GraphConv (None, 16)           784         graph_convolution_96[0][0]       \n",
      "                                                                 input_125[0][0]                  \n",
      "                                                                 input_126[0][0]                  \n",
      "                                                                 input_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_101 (GraphCon (None, 16)           784         graph_convolution_100[0][0]      \n",
      "                                                                 input_129[0][0]                  \n",
      "                                                                 input_130[0][0]                  \n",
      "                                                                 input_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_24 (Gl (None, 64)           0           self__attention_24[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_12 (CoAttLayer)    (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_98 (GraphConv (None, 8)            392         graph_convolution_97[0][0]       \n",
      "                                                                 input_125[0][0]                  \n",
      "                                                                 input_126[0][0]                  \n",
      "                                                                 input_127[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_102 (GraphCon (None, 8)            392         graph_convolution_101[0][0]      \n",
      "                                                                 input_129[0][0]                  \n",
      "                                                                 input_130[0][0]                  \n",
      "                                                                 input_131[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 208)          0           global_average_pooling1d_24[0][0]\n",
      "                                                                 co_att_layer_12[0][0]            \n",
      "                                                                 graph_convolution_98[0][0]       \n",
      "                                                                 graph_convolution_102[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 1)            209         concatenate_12[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,313,015\n",
      "Trainable params: 1,313,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 25s 11ms/step - loss: 0.7510\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.7413\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7314\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.7208\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7094\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6969\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6832\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6681\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6510\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6303\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6058\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5824\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.5621\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5405\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.5170\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.5041\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4923\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4808\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4650\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4605\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4545\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4462\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4414\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.4319\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4275\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4223\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4165\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4107\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4048\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3989\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3903\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3866\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3832\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.3792\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3756\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3752\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3721\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3713\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3679\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3655\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3609\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.3578\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3548\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3519\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3504\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3538\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3524\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3504\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3473\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3479\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3633\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3493\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3531\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3429\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3480\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3415\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3371\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3472\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3349\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3410\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.3288\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.3320\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3244\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3251\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3239\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3203\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3207\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3187\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3154\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3153\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3120\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3112\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3079\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3066\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3041\n",
      "i=4,l={'l1': 0.0, 'l2': 0.0003000000142492354}, lr=0.003, {'acc': '0.838', 'prec': '0.789', 'rec': '0.633', 'f1': '0.697'}\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_139 (InputLayer)          (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_143 (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 16)           1154832     input_139[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 16)           4816        input_143[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 16)           0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_136 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_137 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_138 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 16)           0           dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_140 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_141 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_142 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_134 (InputLayer)          (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_103 (GraphCon (None, 16)           784         dropout_25[0][0]                 \n",
      "                                                                 input_136[0][0]                  \n",
      "                                                                 input_137[0][0]                  \n",
      "                                                                 input_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_107 (GraphCon (None, 16)           784         dropout_26[0][0]                 \n",
      "                                                                 input_140[0][0]                  \n",
      "                                                                 input_141[0][0]                  \n",
      "                                                                 input_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_134[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_135 (InputLayer)          (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_104 (GraphCon (None, 16)           784         graph_convolution_103[0][0]      \n",
      "                                                                 input_136[0][0]                  \n",
      "                                                                 input_137[0][0]                  \n",
      "                                                                 input_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_108 (GraphCon (None, 16)           784         graph_convolution_107[0][0]      \n",
      "                                                                 input_140[0][0]                  \n",
      "                                                                 input_141[0][0]                  \n",
      "                                                                 input_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_26 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, None, 64)     63936       input_135[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_105 (GraphCon (None, 16)           784         graph_convolution_104[0][0]      \n",
      "                                                                 input_136[0][0]                  \n",
      "                                                                 input_137[0][0]                  \n",
      "                                                                 input_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_109 (GraphCon (None, 16)           784         graph_convolution_108[0][0]      \n",
      "                                                                 input_140[0][0]                  \n",
      "                                                                 input_141[0][0]                  \n",
      "                                                                 input_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_26 (Gl (None, 64)           0           self__attention_26[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_13 (CoAttLayer)    (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_106 (GraphCon (None, 8)            392         graph_convolution_105[0][0]      \n",
      "                                                                 input_136[0][0]                  \n",
      "                                                                 input_137[0][0]                  \n",
      "                                                                 input_138[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_110 (GraphCon (None, 8)            392         graph_convolution_109[0][0]      \n",
      "                                                                 input_140[0][0]                  \n",
      "                                                                 input_141[0][0]                  \n",
      "                                                                 input_142[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 208)          0           global_average_pooling1d_26[0][0]\n",
      "                                                                 co_att_layer_13[0][0]            \n",
      "                                                                 graph_convolution_106[0][0]      \n",
      "                                                                 graph_convolution_110[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 1)            209         concatenate_13[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,313,015\n",
      "Trainable params: 1,313,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 30s 14ms/step - loss: 0.7524\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.7291\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6992\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6610\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6137\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 8ms/step - loss: 0.5514\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 18s 8ms/step - loss: 0.4992\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4648\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5248\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4648\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 20s 9ms/step - loss: 0.4963\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4709\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4367\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4391\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4469\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4444\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4277\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4236\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4245\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4193\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4108\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4077\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.4085\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4053\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3981\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3934\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3923\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3892\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3851\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3809\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3732\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3709\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3682\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3656\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3628\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3601\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3568\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3529\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3496\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3461\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3428\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3402\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3374\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3347\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3327\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3405\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3385\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3359\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3333\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3307\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3281\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3256\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3233\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3211\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3189\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3165\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3141\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3118\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.3095\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3072\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3111\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3087\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3062\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3037\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3010\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.2977\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.2944\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.2912\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.2882\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.2850\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.2815\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.2778\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.2747\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.2740\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.2690\n",
      "i=4,l={'l1': 0.0, 'l2': 0.0003000000142492354}, lr=0.01, {'acc': '0.836', 'prec': '0.812', 'rec': '0.605', 'f1': '0.685'}\n",
      "Total run time=12636.652625322342\n"
     ]
    }
   ],
   "source": [
    "ls = [l1(1e-4), l1(3e-4), l1(1e-3), l2(1e-4), l2(3e-4), l2(1e-3)]\n",
    "layers=[3,4,5]\n",
    "lrs = [3e-4, 1e-3, 3e-3, 0.01]\n",
    "\n",
    "results = {}\n",
    "start=time.time()\n",
    "for i in layers:\n",
    "    for l in ls:\n",
    "        for lr in lrs:\n",
    "            clf = HENIN(GCNXss_shape=multi_hot_users.shape[1], \n",
    "                GCNXpp_shape=textFeat_all[:,0,:].shape[1], \n",
    "                reg=l, n_layers=i,lr=lr,\n",
    "                n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN, \n",
    "                MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n",
    "\n",
    "            AvgCP, AvgGP = HENIN_cv(graph=graph, y=y_all, A=ppA, model=clf, epochs=40)\n",
    "            results[(i, l, lr)] = AvgGP\n",
    "            print(f\"i={i},l={l.get_config()}, lr={lr}, {AvgGP}\")\n",
    "print(f\"Total run time={time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "technical-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results.values(),dtype='float')\n",
    "\n",
    "best_params=list(df['f1'].sort_values(ascending=False).index[0:3])\n",
    "\n",
    "best_params=list(np.array([(i, l, lr) for i in layers for l in ls for lr in lrs])[best_params])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "level-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers=4, reg={'l1': 0.0, 'l2': 9.999999747378752e-05}, lr=0.01\n",
      "layers=4, reg={'l1': 0.0003000000142492354, 'l2': 0.0}, lr=0.01\n",
      "layers=4, reg={'l1': 0.0, 'l2': 0.0003000000142492354}, lr=0.003\n"
     ]
    }
   ],
   "source": [
    "for item in best_params:\n",
    "    print(f\"layers={item[0]}, reg={item[1].get_config()}, lr={item[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caroline-return",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_172 (InputLayer)          (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_176 (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 16)           1154832     input_172[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 16)           4816        input_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 16)           0           dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_169 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_170 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_171 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 16)           0           dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_173 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_174 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_175 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_167 (InputLayer)          (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_127 (GraphCon (None, 16)           784         dropout_31[0][0]                 \n",
      "                                                                 input_169[0][0]                  \n",
      "                                                                 input_170[0][0]                  \n",
      "                                                                 input_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_131 (GraphCon (None, 16)           784         dropout_32[0][0]                 \n",
      "                                                                 input_173[0][0]                  \n",
      "                                                                 input_174[0][0]                  \n",
      "                                                                 input_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_167[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_168 (InputLayer)          (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_128 (GraphCon (None, 16)           784         graph_convolution_127[0][0]      \n",
      "                                                                 input_169[0][0]                  \n",
      "                                                                 input_170[0][0]                  \n",
      "                                                                 input_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_132 (GraphCon (None, 16)           784         graph_convolution_131[0][0]      \n",
      "                                                                 input_173[0][0]                  \n",
      "                                                                 input_174[0][0]                  \n",
      "                                                                 input_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_32 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, None, 64)     63936       input_168[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_129 (GraphCon (None, 16)           784         graph_convolution_128[0][0]      \n",
      "                                                                 input_169[0][0]                  \n",
      "                                                                 input_170[0][0]                  \n",
      "                                                                 input_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_133 (GraphCon (None, 16)           784         graph_convolution_132[0][0]      \n",
      "                                                                 input_173[0][0]                  \n",
      "                                                                 input_174[0][0]                  \n",
      "                                                                 input_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_32 (Gl (None, 64)           0           self__attention_32[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_16 (CoAttLayer)    (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_130 (GraphCon (None, 8)            392         graph_convolution_129[0][0]      \n",
      "                                                                 input_169[0][0]                  \n",
      "                                                                 input_170[0][0]                  \n",
      "                                                                 input_171[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_134 (GraphCon (None, 8)            392         graph_convolution_133[0][0]      \n",
      "                                                                 input_173[0][0]                  \n",
      "                                                                 input_174[0][0]                  \n",
      "                                                                 input_175[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 208)          0           global_average_pooling1d_32[0][0]\n",
      "                                                                 co_att_layer_16[0][0]            \n",
      "                                                                 graph_convolution_130[0][0]      \n",
      "                                                                 graph_convolution_134[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 1)            209         concatenate_16[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,313,015\n",
      "Trainable params: 1,313,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 42s 19ms/step - loss: 0.7382\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-7896071db540>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mAvgCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAvgGP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHENIN_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mppA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mbest_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAvgGP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"i={param[0]},l={param[1].get_config()}, lr={param[2]}, {AvgGP}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-56bfff179fda>\u001b[0m in \u001b[0;36mHENIN_cv\u001b[1;34m(graph, y, A, model, epochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[1;31m#if epoch%5==0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;31m#print(metrics(y[test_index], (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)[test_index]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[1;32m--> 185\u001b[1;33m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_results={}\n",
    "\n",
    "#start=time.time()\n",
    "for param in best_params[::-1]:\n",
    "    clf = HENIN(GCNXss_shape=multi_hot_users.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all[:,0,:].shape[1], \n",
    "            reg=param[1], n_layers=param[0], lr=param[2],\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n",
    "\n",
    "    AvgCP, AvgGP = HENIN_cv(graph=graph, y=y_all, A=ppA, model=clf, epochs=10)\n",
    "    best_results[tuple(param)] = AvgGP\n",
    "    print(f\"i={param[0]},l={param[1].get_config()}, lr={param[2]}, {AvgGP}\")\n",
    "#print(f\"Total run time={time.time()-start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "permanent-riverside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_183 (InputLayer)          (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_187 (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 16)           1154832     input_183[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 16)           4816        input_187[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 16)           0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_180 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_181 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_182 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 16)           0           dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_184 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_185 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_186 (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_178 (InputLayer)          (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_135 (GraphCon (None, 16)           784         dropout_33[0][0]                 \n",
      "                                                                 input_180[0][0]                  \n",
      "                                                                 input_181[0][0]                  \n",
      "                                                                 input_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_139 (GraphCon (None, 16)           784         dropout_34[0][0]                 \n",
      "                                                                 input_184[0][0]                  \n",
      "                                                                 input_185[0][0]                  \n",
      "                                                                 input_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       57600       input_178[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_179 (InputLayer)          (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_136 (GraphCon (None, 16)           784         graph_convolution_135[0][0]      \n",
      "                                                                 input_180[0][0]                  \n",
      "                                                                 input_181[0][0]                  \n",
      "                                                                 input_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_140 (GraphCon (None, 16)           784         graph_convolution_139[0][0]      \n",
      "                                                                 input_184[0][0]                  \n",
      "                                                                 input_185[0][0]                  \n",
      "                                                                 input_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_34 (Self_Attent (None, 75, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, None, 64)     63936       input_179[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_137 (GraphCon (None, 16)           784         graph_convolution_136[0][0]      \n",
      "                                                                 input_180[0][0]                  \n",
      "                                                                 input_181[0][0]                  \n",
      "                                                                 input_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_141 (GraphCon (None, 16)           784         graph_convolution_140[0][0]      \n",
      "                                                                 input_184[0][0]                  \n",
      "                                                                 input_185[0][0]                  \n",
      "                                                                 input_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_34 (Gl (None, 64)           0           self__attention_34[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_17 (CoAttLayer)    (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_138 (GraphCon (None, 8)            392         graph_convolution_137[0][0]      \n",
      "                                                                 input_180[0][0]                  \n",
      "                                                                 input_181[0][0]                  \n",
      "                                                                 input_182[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_142 (GraphCon (None, 8)            392         graph_convolution_141[0][0]      \n",
      "                                                                 input_184[0][0]                  \n",
      "                                                                 input_185[0][0]                  \n",
      "                                                                 input_186[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 208)          0           global_average_pooling1d_34[0][0]\n",
      "                                                                 co_att_layer_17[0][0]            \n",
      "                                                                 graph_convolution_138[0][0]      \n",
      "                                                                 graph_convolution_142[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 1)            209         concatenate_17[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,313,015\n",
      "Trainable params: 1,313,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 33s 15ms/step - loss: 0.7240\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6952\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.6664\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.6267\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5889\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.5421\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4909\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4678\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4413\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4535\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.4376\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 7ms/step - loss: 0.4271\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3942\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.4023\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3860\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3715\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3745\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3754\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3717\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3702\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3677\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3618\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3622\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3566\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3515\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 19s 9ms/step - loss: 0.3504\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3497\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3483\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3466\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3438\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 17s 8ms/step - loss: 0.3408\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3393\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 16s 7ms/step - loss: 0.3373\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8adcc7788ea8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mAvgCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAvgGP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHENIN_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mppA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#best_results[tuple(param)] = AvgGP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\" result={AvgGP}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-56bfff179fda>\u001b[0m in \u001b[0;36mHENIN_cv\u001b[1;34m(graph, y, A, model, epochs)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[1;31m#if epoch%5==0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;31m#print(metrics(y[test_index], (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)[test_index]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m                         \u001b[1;34m'Feeding from symbolic tensors is not '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m                         'supported with sparse inputs.')\n\u001b[1;32m-> 2703\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m             \u001b[1;31m# callable generated by Session._make_callable_from_options accepts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2693\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2694\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = HENIN(GCNXss_shape=multi_hot_users.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all[:,0,:].shape[1], \n",
    "            reg=l2(1e-4), n_layers=4, lr=0.01,\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n",
    "\n",
    "AvgCP, AvgGP = HENIN_cv(graph=graph, y=y_all, A=ppA, model=clf, epochs=60)\n",
    "#best_results[tuple(param)] = AvgGP\n",
    "print(f\" result={AvgGP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "little-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers=4, reg={'l1': 0.0, 'l2': 9.999999747378752e-05}, result={'acc': '0.893', 'prec': '0.847', 'rec': '0.789', 'f1': '0.816'}\n",
      "layers=5, reg={'l1': 0.0003000000142492354, 'l2': 0.0}, result={'acc': '0.879', 'prec': '0.817', 'rec': '0.778', 'f1': '0.797'}\n",
      "layers=4, reg={'l1': 0.0, 'l2': 0.0003000000142492354}, result={'acc': '0.886', 'prec': '0.832', 'rec': '0.787', 'f1': '0.808'}\n"
     ]
    }
   ],
   "source": [
    "best_results\n",
    "for k,v in best_results.items():\n",
    "    print(f\"layers={k[0]}, reg={k[1].get_config()}, result={v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "sapphire-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_params.pkl','wb') as f:\n",
    "    pickle.dump(best_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "coastal-heater",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "best_params = clf.to_json()\n",
    "with open(\"best_model.json\", \"w\") as json_file:\n",
    "    json_file.write(best_params)\n",
    "# serialize weights to HDF5\n",
    "clf.save_weights(\"best_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-compact",
   "metadata": {},
   "source": [
    "# Early Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "casual-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1 3rd of comments\n",
    "'''\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4ModelINSTA25.pickle', 'rb') as f:\n",
    "    Dat4Model_3rd = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_usersINSTA25.pickle', 'rb') as f:\n",
    "    multi_hot_users_3rd = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all_3rd = Dat4Model_3rd['w2v_vec_all'] # features for HENIN\n",
    "y_all_3rd = Dat4Model_3rd['y_all'] # target for HENIN\n",
    "textFeat_all_3rd = Dat4Model_3rd['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN_3rd = w2v_vec_all_3rd.shape[2]\n",
    "MAX_REV_LEN_3rd = w2v_vec_all_3rd.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb_3rd = pad_sequences(w2v_vec_all_3rd[:,0,:,:], maxlen=MAX_REV_LEN_3rd, dtype='float32', padding='post') \n",
    "\n",
    "ppA_3rd = genAdjacencyMatrix(textFeat_all_3rd[:,0,:], 'cosine')\n",
    "ssA_3rd = genAdjacencyMatrix(multi_hot_users_3rd, 'cosine')\n",
    "\n",
    "graph_ss_3rd = genGCNgraph(ssA_3rd, multi_hot_users_3rd)\n",
    "graph_pp_3rd = genGCNgraph(ppA_3rd, textFeat_all_3rd[:,0,:])\n",
    "\n",
    "graph_3rd = [w2v_vec_all_3rd]+[postEmb_3rd]+graph_ss_3rd+graph_pp_3rd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "latest-outline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2211, 25, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFeat_all_3rd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "disturbed-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 6th of comments\n",
    "\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4ModelINSTA12.pickle', 'rb') as f:\n",
    "    Dat4Model_6th = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_usersINSTA12.pickle', 'rb') as f:\n",
    "    multi_hot_users_6th = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all_6th = Dat4Model_6th['w2v_vec_all'] # features for HENIN\n",
    "y_all_6th = Dat4Model_6th['y_all'] # target for HENIN\n",
    "textFeat_all_6th = Dat4Model_6th['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN_6th = w2v_vec_all_6th.shape[2]\n",
    "MAX_REV_LEN_6th = w2v_vec_all_6th.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb_6th = pad_sequences(w2v_vec_all_6th[:,0,:,:], maxlen=MAX_REV_LEN_6th, dtype='float32', padding='post') \n",
    "\n",
    "ppA_6th = genAdjacencyMatrix(textFeat_all_6th[:,0,:], 'cosine')\n",
    "ssA_6th = genAdjacencyMatrix(multi_hot_users_6th, 'cosine')\n",
    "\n",
    "graph_ss_6th = genGCNgraph(ssA_6th, multi_hot_users_6th)\n",
    "graph_pp_6th = genGCNgraph(ppA_6th, textFeat_all_6th[:,0,:])\n",
    "\n",
    "graph_6th = [w2v_vec_all_6th]+[postEmb_6th]+graph_ss_6th+graph_pp_6th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worst-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           1154832     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           4816        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 25, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 16)           784         dropout_1[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_5 (GraphConvo (None, 16)           784         dropout_2[0][0]                  \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 25, 64)       57600       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 16)           784         graph_convolution_1[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_6 (GraphConvo (None, 16)           784         graph_convolution_5[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_2 (Self_Attenti (None, 25, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 64)     63936       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_3 (GraphConvo (None, 16)           784         graph_convolution_2[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_7 (GraphConvo (None, 16)           784         graph_convolution_6[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           self__attention_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_1 (CoAttLayer)     (None, 128)          7346        word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_4 (GraphConvo (None, 8)            392         graph_convolution_3[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_8 (GraphConvo (None, 8)            392         graph_convolution_7[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 208)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 co_att_layer_1[0][0]             \n",
      "                                                                 graph_convolution_4[0][0]        \n",
      "                                                                 graph_convolution_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            209         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,306,515\n",
      "Trainable params: 1,306,515\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 15s 7ms/step - loss: 0.7028\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.6695\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.6469\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.6266\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.5951\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.5592\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.5276\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.4967\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.4523\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.4445\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.4137\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.4171\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3946\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.4205\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3929\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.4035\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3969\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3865\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3926\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3825\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3751\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3780\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3692\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3654\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3676\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3627\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3596\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3611\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3584\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3549\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3549\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3522\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3482\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3476\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3450\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3424\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3425\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3397\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3397\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3369\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3480\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3467\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3435\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3428\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3403\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3393\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3370\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3353\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3334\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3314\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3295\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3271\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3247\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3218\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3196\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3194\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3171\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3120\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3092\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3083\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3020\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3010\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3078\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.3021\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2918\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2960\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2827\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2884\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2762\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2754\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2677\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2621\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2778\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2977\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2672\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2893\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2653\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2801\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2597\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2630\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2895\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2807\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2806\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2726\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2666\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2605\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2589\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2513\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2488\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2406\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2371\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2298\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2244\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2171\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2117\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2068\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2002\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1955\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1861\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1834\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1748\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1698\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1675\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1648\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1565\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1525\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1623\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1623\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1697\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1913\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1837\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1550\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1736\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1491\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1503\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1502\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1334\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1365\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1396\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1930\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2140\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1980\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1813\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1805\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1766\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1719\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1634\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1574\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1535\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1415\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1352\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1291\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1248\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1174\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1145\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1071\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1038\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0968\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0942\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0868\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0848\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0793\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0777\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0714\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0673\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0625\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0632\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0672\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0621\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0614\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0569\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0531\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0508\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0470\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0443\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0404\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0385\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0365\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0343\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0855\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0762\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0664\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0605\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0552\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0490\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0453\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0413\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0381\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0358\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0330\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0310\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0299\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1805\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2293\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.2610\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1782\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1851\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1428\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1489\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1312\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1199\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1092\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.1003\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0925\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0809\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0748\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0668\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0643\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0603\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0557\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0521\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0491\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0456\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0427\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0397\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0369\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0312\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 13s 6ms/step - loss: 0.0294\n",
      " result={'acc': '0.876', 'prec': '0.826', 'rec': '0.755', 'f1': '0.788'}\n"
     ]
    }
   ],
   "source": [
    "clf_3rd = HENIN(GCNXss_shape=multi_hot_users_3rd.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all_3rd[:,0,:].shape[1], \n",
    "            reg=l2(1e-4), n_layers=4, lr=0.01,\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN_3rd, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN_3rd, support=3)\n",
    "\n",
    "AvgCP_3rd, AvgGP_3rd = HENIN_cv(graph=graph_3rd, y=y_all_3rd, A=ppA_3rd, model=clf_3rd, epochs=40)\n",
    "#best_results[tuple(param)] = AvgGP\n",
    "print(f\" result={AvgGP_3rd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "portable-justice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 72176)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 16)           1154832     input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           4816        input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 16)           0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16)           0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           (None, 12, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_9 (GraphConvo (None, 16)           784         dropout_3[0][0]                  \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_13 (GraphConv (None, 16)           784         dropout_4[0][0]                  \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 12, 64)       57600       input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_10 (GraphConv (None, 16)           784         graph_convolution_9[0][0]        \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_14 (GraphConv (None, 16)           784         graph_convolution_13[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "self__attention_4 (Self_Attenti (None, 12, 64)       12288       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, None, 64)     63936       input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_11 (GraphConv (None, 16)           784         graph_convolution_10[0][0]       \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_15 (GraphConv (None, 16)           784         graph_convolution_14[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 64)           0           self__attention_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_2 (CoAttLayer)     (None, 128)          5656        word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_12 (GraphConv (None, 8)            392         graph_convolution_11[0][0]       \n",
      "                                                                 input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "                                                                 input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_16 (GraphConv (None, 8)            392         graph_convolution_15[0][0]       \n",
      "                                                                 input_19[0][0]                   \n",
      "                                                                 input_20[0][0]                   \n",
      "                                                                 input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 208)          0           global_average_pooling1d_4[0][0] \n",
      "                                                                 co_att_layer_2[0][0]             \n",
      "                                                                 graph_convolution_12[0][0]       \n",
      "                                                                 graph_convolution_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            209         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,304,825\n",
      "Trainable params: 1,304,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 14s 6ms/step - loss: 0.7076\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.6710\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.6511\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.6367\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.6122\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.5862\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.5586\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.5325\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.5076\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4882\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4824\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4712\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4611\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4519\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4459\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4415\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4377\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4326\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4294\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4251\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4235\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4210\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4192\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4177\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4150\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4132\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 6ms/step - loss: 0.4101\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 6ms/step - loss: 0.4088\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 6ms/step - loss: 0.4067\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4056\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4038\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.4023\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3999\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3980\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3954\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3937\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3913\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3894\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3874\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 6ms/step - loss: 0.3847\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3918\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3904\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3878\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3852\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3820\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3796\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3751\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3706\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3674\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3648\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3604\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3569\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3496\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3461\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3407\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3377\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3390\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3267\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3183\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3140\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.3108\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2988\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2906\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2839\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2861\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2674\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2549\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2502\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2461\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2495\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2328\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2165\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2191\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2116\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1933\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1851\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1845\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1668\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1577\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1552\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2610\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2471\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2409\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2367\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2230\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2002\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1921\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1821\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1712\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1628\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1516\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1430\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1356\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1272\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1200\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1119\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1053\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0964\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0833\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0775\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0729\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0686\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0646\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0840\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0859\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0813\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0746\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0706\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0655\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0603\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0541\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0520\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0496\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0461\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0433\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0410\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0372\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0354\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0320\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1188\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1129\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0859\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0852\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0822\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0719\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0653\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0585\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0520\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0509\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0463\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0402\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0396\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0366\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0358\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0313\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0324\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0289\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0239\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0256\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0219\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0199\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0168\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0163\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0149\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0144\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0123\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 6ms/step - loss: 0.0124\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0112\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0100\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0092\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0084\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0077\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0071\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0067\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0062\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0056\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0051\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0046\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0043\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0675\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0437\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0383\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0408\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0359\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0222\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0268\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0444\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0603\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1437\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2816\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0761\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2013\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0559\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.2406\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0583\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1562\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1293\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1049\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.1241\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0513\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0431\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0658\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0711\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0545\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0577\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0483\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0321\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0264\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0309\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0342\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0260\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0227\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0236\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0213\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0170\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0128\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0113\n",
      "Epoch 1/1\n",
      "2211/2211 [==============================] - 12s 5ms/step - loss: 0.0114\n",
      " result={'acc': '0.872', 'prec': '0.809', 'rec': '0.753', 'f1': '0.778'}\n"
     ]
    }
   ],
   "source": [
    "clf_6th = HENIN(GCNXss_shape=multi_hot_users_6th.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all_6th[:,0,:].shape[1], \n",
    "            reg=l2(1e-4), n_layers=4, lr=0.01,\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN_6th, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN_6th, support=3)\n",
    "\n",
    "AvgCP_6th, AvgGP_6th = HENIN_cv(graph=graph_6th, y=y_all_6th, A=ppA_6th, model=clf_6th, epochs=40)\n",
    "#best_results[tuple(param)] = AvgGP\n",
    "print(f\" result={AvgGP_6th}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
