{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "velvet-recorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pydot\n",
    "from collections import defaultdict\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, GlobalAveragePooling1D, GRU, Bidirectional\n",
    "from keras.layers import GlobalMaxPooling1D, LSTM, Dropout, SimpleRNN, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import concatenate\n",
    "from keras import activations, initializers, constraints\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l1,l2, l1_l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "tf.keras.backend.set_session(sess)\n",
    "\n",
    "from layers import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latest-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace self-attention layers with LSTM layers\n",
    "def HENIN_LSTM(GCNXss_shape, GCNXpp_shape, reg=l2(5e-4), n_layers=2, n_head=8, size_per_head=8, MAX_REV_LEN=75, MAX_REV_WORD_LEN=10, support=3):\n",
    "    \n",
    "    '''\n",
    "    Comment Encoding\n",
    "    '''\n",
    "    \n",
    "    ''' Capture reviews context correlation'''\n",
    "    ## word-level encoding\n",
    "    word_input = Input(shape=(None, 300), dtype='float32')\n",
    "    #word_sa = Self_Attention(n_head, size_per_head)(word_input)\n",
    "    word_sa = LSTM(n_head*size_per_head, return_sequences=True)(word_input)\n",
    "    word_avg = GlobalAveragePooling1D()(word_sa)\n",
    "    wordEncoder = Model(word_input, word_avg)\n",
    "    \n",
    "    ## review-level encoding\n",
    "    content_input = Input(shape=(MAX_REV_LEN, MAX_REV_WORD_LEN, 300), dtype='float32')\n",
    "    content_word_encode = TimeDistributed(wordEncoder, name='word_seq_encoder')(content_input)\n",
    "    #content_sa = Self_Attention(n_head, size_per_head)(content_word_encode)\n",
    "    content_sa = LSTM(n_head*size_per_head, return_sequences=True)(content_word_encode)\n",
    "    contentSA_avg_pool = GlobalAveragePooling1D()(content_sa) # session embedding\n",
    "    \n",
    "    ''' Capture Post-Comment co-attention'''\n",
    "    post_words_input = Input(shape=(None, 300), dtype='float32')\n",
    "    post_lstm = Bidirectional(GRU(32, return_sequences=True))(post_words_input)\n",
    "    coAtt_vec = CoAttLayer(MAX_REV_LEN)([content_word_encode, post_lstm])\n",
    "    \n",
    "    '''\n",
    "    GCN\n",
    "    Session-Session Interaction Extractor\n",
    "    Adjacency: session-session\n",
    "    '''\n",
    "    G_ss = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(3)]\n",
    "    \n",
    "    X_ss = Input(shape=(GCNXss_shape,))\n",
    "    X_ss_emb = Dense(16, activation='relu')(X_ss)\n",
    "    \n",
    "    # Define GCN model architecture\n",
    "    H_ss = Dropout(0.2)(X_ss_emb)\n",
    "    for i in range(n_layers-1):\n",
    "        H_ss = GraphConvolution(16, support, activation='relu', kernel_regularizer=reg)([H_ss]+G_ss)\n",
    "        \n",
    "    H_ss = GraphConvolution(8, support, activation='softmax', kernel_regularizer=reg)([H_ss]+G_ss)\n",
    "    \n",
    "    '''\n",
    "    GCN\n",
    "    Post-Post Interaction Extractor\n",
    "    Adjacency: post-post\n",
    "    '''\n",
    "    G_pp = [Input(shape=(None, None), batch_shape=(None, None), sparse=True) for _ in range(3)]\n",
    "    \n",
    "    X_pp = Input(shape=(GCNXpp_shape,))\n",
    "    X_pp_emb = Dense(16, activation='relu')(X_pp)\n",
    "    \n",
    "    # Define GCN model architecture\n",
    "    H_pp = Dropout(0.2)(X_pp_emb)\n",
    "    for i in range(n_layers-1):\n",
    "        H_pp = GraphConvolution(16, support, activation='relu', kernel_regularizer=reg)([H_pp]+G_pp)\n",
    "    H_pp = GraphConvolution(8, support, activation='softmax', kernel_regularizer=reg)([H_pp]+G_pp)\n",
    "     \n",
    "    '''\n",
    "    Concatenate Comment Encoding & GCN Embedding\n",
    "    '''\n",
    "    H = concatenate([contentSA_avg_pool, coAtt_vec, H_ss, H_pp])\n",
    "    Y = Dense(1, activation='sigmoid')(H)\n",
    "    \n",
    "    # Compile model\n",
    "    model = Model(inputs=[content_input]+[post_words_input]+[X_ss]+G_ss+[X_pp]+G_pp, outputs=Y)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01))\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acceptable-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load data\n",
    "'''\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4ModelVINE.pickle', 'rb') as f:\n",
    "    Dat4Model = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_usersVINE.pickle', 'rb') as f:\n",
    "    multi_hot_users = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all = Dat4Model['w2v_vec_all'] # features for HENIN\n",
    "y_all = Dat4Model['y_all'] # target for HENIN\n",
    "textFeat_all = Dat4Model['textFeat_all']\n",
    "\n",
    "MAX_REV_WORD_LEN = w2v_vec_all.shape[2]\n",
    "MAX_REV_LEN = w2v_vec_all.shape[1]\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb = pad_sequences(w2v_vec_all[:,0,:,:], maxlen=MAX_REV_LEN, dtype='float32', padding='post') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "governmental-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross validating for HENIN model\n",
    "def HENIN_cv(graph, y, A, model, epochs):\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=9999, shuffle=True)\n",
    "    iters = 0\n",
    "    \n",
    "    for train_index, test_index in skf.split(range(len(y)), y):\n",
    "        y_train, y_test, train_mask = Mask_y(y=y, train_ix=train_index, test_ix=test_index)\n",
    "        #y_train, y_test = Mask_y(y=y, train_ix=train_index, test_ix=test_index)\n",
    "        clf = model\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            clf.fit(graph, y_train, sample_weight=train_mask, batch_size=A.shape[0], epochs=1)\n",
    "            #if epoch%5==0:\n",
    "                #print(metrics(y[test_index], (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)[test_index]))\n",
    "        preds = (clf.predict(graph, batch_size=A.shape[0])[:,0] >= 0.5).astype(int)\n",
    "        \n",
    "        completePerform = metrics(y, preds) # Complete set performance\n",
    "        generalPerform = metrics(y[test_index], preds[test_index]) # test set performance\n",
    "        \n",
    "          \n",
    "        try:\n",
    "            if iters == 1:\n",
    "                CP = {k: v + [completePerform[k]] for k, v in CP.items()}\n",
    "                GP = {k: v + [generalPerform[k]] for k, v in GP.items()}\n",
    "            else:  \n",
    "                CP = {k: [v] + [completePerform[k]] for k, v in CP.items()}\n",
    "                GP = {k: [v] + [generalPerform[k]] for k, v in GP.items()}\n",
    "                iters += 1\n",
    "        except:\n",
    "            CP = completePerform\n",
    "            GP = generalPerform\n",
    "    \n",
    "    AvgCP = {k: '{:.3f}'.format(np.mean(v)) for k, v in CP.items()}\n",
    "    AvgGP = {k: '{:.3f}'.format(np.mean(v)) for k, v in GP.items()}\n",
    "    \n",
    "    return AvgCP, AvgGP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "substantial-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-params tuning\n",
    " \n",
    "import time\n",
    "\n",
    "ppA = genAdjacencyMatrix(textFeat_all[:,0,:], 'cosine')\n",
    "ssA = genAdjacencyMatrix(multi_hot_users, 'cosine')\n",
    "\n",
    "graph_ss = genGCNgraph(ssA, multi_hot_users)\n",
    "graph_pp = genGCNgraph(ppA, textFeat_all[:,0,:])\n",
    "\n",
    "graph = [w2v_vec_all]+[postEmb]+graph_ss+graph_pp\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "caroline-return",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 29051)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           464832      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           4816        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 16)           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 75, 10, 300)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 16)           784         dropout_1[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_5 (GraphConvo (None, 16)           784         dropout_2[0][0]                  \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_seq_encoder (TimeDistribut (None, 75, 64)       93440       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None, 300)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 16)           784         graph_convolution_1[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_6 (GraphConvo (None, 16)           784         graph_convolution_5[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 75, 64)       33024       word_seq_encoder[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, None, 64)     63936       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_3 (GraphConvo (None, 16)           784         graph_convolution_2[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_7 (GraphConvo (None, 16)           784         graph_convolution_6[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 64)           0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "co_att_layer_1 (CoAttLayer)     (None, 128)          13846       word_seq_encoder[0][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_4 (GraphConvo (None, 8)            392         graph_convolution_3[0][0]        \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_8 (GraphConvo (None, 8)            392         graph_convolution_7[0][0]        \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 208)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 co_att_layer_1[0][0]             \n",
      "                                                                 graph_convolution_4[0][0]        \n",
      "                                                                 graph_convolution_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            209         concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 679,591\n",
      "Trainable params: 679,591\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\weili\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "967/967 [==============================] - 9s 10ms/step - loss: 0.7823\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.7386\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.7058\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.6671\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.7611\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.7292\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.6392\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.6251\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.6439\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.6180\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 6s 7ms/step - loss: 0.6029\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 6s 7ms/step - loss: 0.6038\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5934\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5687\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5731\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5500\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5458\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5387\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 6s 7ms/step - loss: 0.5232\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5214\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5221\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5020\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5118\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5083\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4946\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4997\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4958\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4903\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4944\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4871\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4869\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4883\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4809\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4802\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4868\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4876\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4815\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4728\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4796\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4760\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4614\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4720\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4633\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4577\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4675\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4651\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4554\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4657\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4570\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4599\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4595\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4543\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4595\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4512\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4565\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4499\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4511\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4512\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 6s 7ms/step - loss: 0.4465\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4503\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4468\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4455\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4479\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4435\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4424\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4459\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4473\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4458\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4413\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4428\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4385\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4435\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4384\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4366\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4409\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4352\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4350\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 6s 7ms/step - loss: 0.4529\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4567\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4492\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4359\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4409\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4349\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4404\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4310\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4376\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4366\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4350\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4793\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4447\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4687\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4562\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4423\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4535\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4326\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4447\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4349\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4438\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 6s 7ms/step - loss: 0.4343\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4366\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4340\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4299\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4318\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4553\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4415\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4434\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4540\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4360\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4460\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4390\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4329\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4413\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4287\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4351\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4283\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4328\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4286\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4272\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4274\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4410\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4365\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4357\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4361\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4364\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4364\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4363\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4350\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4348\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4340\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4334\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4324\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4314\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4314\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4307\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4303\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4299\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4311\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4356\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4286\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4293\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4289\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4314\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4292\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4276\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4259\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4246\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4239\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4251\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4252\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4239\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4220\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4209\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4203\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4189\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4172\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4161\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4154\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.6005\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.6248\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 6s 7ms/step - loss: 0.6125\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5399\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5088\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.5144\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4873\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4948\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4877\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4620\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4851\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4791\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4689\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4915\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4576\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4903\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4631\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4709\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4828\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4631\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4620\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4836\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4565\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4632\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4793\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4496\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 6s 7ms/step - loss: 0.4579\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4553\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4482\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4491\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4478\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4460\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4451\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4430\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4412\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4429\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4369\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4404\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4389\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4375\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4372\n",
      "Epoch 1/1\n",
      "967/967 [==============================] - 7s 7ms/step - loss: 0.4342\n",
      "{'acc': '0.778', 'prec': '0.717', 'rec': '0.503', 'f1': '0.563'}\n"
     ]
    }
   ],
   "source": [
    "clf = HENIN_LSTM(GCNXss_shape=multi_hot_users.shape[1], \n",
    "\t        GCNXpp_shape=textFeat_all[:,0,:].shape[1], \n",
    "            reg=l2(5e-4), n_layers=4,\n",
    "\t        n_head=8, size_per_head=8, MAX_REV_LEN=MAX_REV_LEN, \n",
    "\t        MAX_REV_WORD_LEN=MAX_REV_WORD_LEN, support=3)\n",
    "\n",
    "AvgCP, AvgGP = HENIN_cv(graph=graph, y=y_all, A=ppA, model=clf, epochs=40)\n",
    "print(AvgGP)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "little-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers=4, reg={'l1': 0.0, 'l2': 9.999999747378752e-05}, result={'acc': '0.794', 'prec': '0.754', 'rec': '0.523', 'f1': '0.613'}\n",
      "layers=5, reg={'l1': 0.0003000000142492354, 'l2': 0.0}, result={'acc': '0.805', 'prec': '0.739', 'rec': '0.586', 'f1': '0.651'}\n",
      "layers=4, reg={'l1': 0.0, 'l2': 0.0003000000142492354}, result={'acc': '0.801', 'prec': '0.733', 'rec': '0.579', 'f1': '0.644'}\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "lstm_layer = clf.to_json()\n",
    "with open(\"vine_lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(lstm_layer)\n",
    "# serialize weights to HDF5\n",
    "clf.save_weights(\"vine_lstm_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sapphire-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load Test data\n",
    "'''\n",
    "# load preprocessed data\n",
    "with open('preprocessData/Dat4ModelUNLAB.pickle', 'rb') as f:\n",
    "    Dat4Model_test = pickle.load(f)\n",
    "    \n",
    "# load multi-hot user vectors of each session\n",
    "with open('preprocessData/multi_hot_usersUNLAB.pickle', 'rb') as f:\n",
    "    multi_hot_users_test = pickle.load(f)  \n",
    "    \n",
    "w2v_vec_all_test = Dat4Model_test['w2v_vec_all'] # features for HENIN\n",
    "y_all_test = Dat4Model_test['y_all'] # target for HENIN\n",
    "textFeat_all_test = Dat4Model_test['textFeat_all']\n",
    "\n",
    "# word embedding of posted text\n",
    "postEmb_test = pad_sequences(w2v_vec_all_test[:,0,:,:], maxlen=MAX_REV_LEN, dtype='float32', padding='post') \n",
    "\n",
    "\n",
    "ppA_test = genAdjacencyMatrix(textFeat_all_test[:,0,:], 'cosine')\n",
    "ssA_test = genAdjacencyMatrix(multi_hot_users_test, 'cosine')\n",
    "\n",
    "graph_ss_test = genGCNgraph(ssA_test, multi_hot_users_test)\n",
    "graph_pp_test = genGCNgraph(ppA_test, textFeat_all_test[:,0,:])\n",
    "\n",
    "graph_test = [w2v_vec_all_test]+[postEmb_test]+graph_ss_test+graph_pp_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "antique-drama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217, 75, 10, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "secure-change",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(967, 75, 10, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
